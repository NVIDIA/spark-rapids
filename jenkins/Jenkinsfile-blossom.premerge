#!/usr/local/env groovy
/*
 * Copyright (c) 2020-2022, NVIDIA CORPORATION.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 *
 * Jenkinsfile for building rapids-plugin on blossom
 *
 */
import hudson.model.Result
import hudson.model.Run
import jenkins.model.CauseOfInterruption.UserInterruption

@Library(['shared-libs', 'blossom-lib']) _
@Library('blossom-github-lib@master')
import ipp.blossom.*

def githubHelper // blossom github helper
def TEMP_IMAGE_BUILD = true
def CUDA_NAME = 'cuda11.0' // hardcode cuda version for docker build part
def PREMERGE_DOCKERFILE = 'jenkins/Dockerfile-blossom.ubuntu'
def IMAGE_PREMERGE // temp image for premerge test
def IMAGE_DB = pod.getCPUYAML("${common.ARTIFACTORY_NAME}/sw-spark-docker/spark:rapids-databricks")
def PREMERGE_TAG
def skipped = false
def db_build = false
def sourcePattern = 'shuffle-plugin/src/main/scala/,udf-compiler/src/main/scala/,' +
    'sql-plugin/src/main/java/,sql-plugin/src/main/scala/'

// constant parameters for aws and azure databricks cluster
// CSP params
ID_HOST = 0
ID_TOKEN = 1
ID_DRIVER = 2
ID_WORKER = 3
PARAM_MAP = [
        'aws'  : [
                "${common.AWS_DATABRICKS_URL}",
                'SPARK_AWS_DATABRICKS_TOKEN',
                'g4dn.2xlarge',
                'g4dn.2xlarge'
        ],
        'azure': [
                "${common.AZURE_DATABRICKS_URL}",
                'SPARK_AZURE_DATABRICKS_TOKEN',
                'Standard_NC6s_v3',
                'Standard_NC6s_v3'
        ]
]
// runtime params
ID_RUNTIME = 0
ID_SPARK = 1
ID_INITSCRIPTS = 2
ID_INSTALL = 3
RUNTIME_MAP = [
        '9.1': [
                '9.1.x-gpu-ml-scala2.12',
                '3.1.2',
                'init_cudf_udf.sh',
                '3.1.2'
        ],
        '10.4': [
                '10.4.x-gpu-ml-scala2.12',
                '3.2.1',
                'init_cudf_udf.sh',
                '3.2.1'
        ],
        '11.3': [
                '11.3.x-gpu-ml-scala2.12',
                '3.3.0',
                'init_cudf_udf.sh',
                '3.3.0'
        ]
]

pipeline {
    agent {
        kubernetes {
            label "premerge-init-${BUILD_TAG}"
            cloud 'sc-ipp-blossom-prod'
            yaml "${IMAGE_DB}"
        }
    }

    options {
        ansiColor('xterm')
        buildDiscarder(logRotator(numToKeepStr: '50'))
        skipDefaultCheckout true
        timeout(time: 12, unit: 'HOURS')
    }

    parameters {
        string(name: 'REF', defaultValue: '',
            description: 'Merged commit of specific PR')
        string(name: 'GITHUB_DATA', defaultValue: '',
            description: 'Json-formatted github data from upstream blossom-ci')
    }

    environment {
        JENKINS_ROOT = 'jenkins'
        PREMERGE_SCRIPT = '$JENKINS_ROOT/spark-premerge-build.sh'
        MVN_URM_MIRROR = '-s jenkins/settings.xml -P mirror-apache-to-urm'
        LIBCUDF_KERNEL_CACHE_PATH = '/tmp/.cudf'
        ARTIFACTORY_NAME = "${ArtifactoryConstants.ARTIFACTORY_NAME}"
        GITHUB_TOKEN = credentials("github-token")
        URM_CREDS = credentials("urm_creds")
        URM_URL = "https://${ArtifactoryConstants.ARTIFACTORY_NAME}/artifactory/sw-spark-maven"
        PVC = credentials("pvc")
        CUSTOM_WORKSPACE = "/home/jenkins/agent/workspace/${BUILD_TAG}"
        CUDA_CLASSIFIER = 'cuda11'
        // DB related ENVs
        IDLE_TIMEOUT = '240' // 4 hours
        NUM_WORKERS = '0'
        DB_TYPE = getDbType()
        DATABRICKS_HOST = "${PARAM_MAP["$DB_TYPE"][ID_HOST]}"
        DATABRICKS_TOKEN = credentials("${PARAM_MAP["$DB_TYPE"][ID_TOKEN]}")
        DATABRICKS_PUBKEY = credentials("SPARK_DATABRICKS_PUBKEY")
        DATABRICKS_DRIVER = "${PARAM_MAP["$DB_TYPE"][ID_DRIVER]}"
        DATABRICKS_WORKER = "${PARAM_MAP["$DB_TYPE"][ID_WORKER]}"
        INIT_SCRIPTS_DIR = "dbfs:/databricks/init_scripts/${BUILD_TAG}"
    }

    stages {
        stage("Init githubHelper") {
            steps {
                script {
                    githubHelper = GithubHelper.getInstance("${GITHUB_TOKEN}", params.GITHUB_DATA)
                    // desc contains the PR ID and can be accessed from different builds
                    currentBuild.description = githubHelper.getBuildDescription()
                    try {
                        // quiet period here in case the first build of two close dup triggers has not set desc
                        sleep(time: 30, unit: "SECONDS")
                        // abort duplicate running builds of the same PR (based on build desc)
                        abortDupBuilds()
                    } catch (e) { // do not block following build if abort failure
                        echo "failed to try abort duplicate builds: " + e.toString()
                    }

                    def title = githubHelper.getIssue().title.toLowerCase()
                    if (title ==~ /.*\[skip ci\].*/) {
                        githubHelper.updateCommitStatus("$BUILD_URL", "Skipped", GitHubCommitState.SUCCESS)
                        currentBuild.result == "SUCCESS"
                        skipped = true
                        return
                    }
                    checkoutCode(githubHelper.getCloneUrl(), githubHelper.getMergedSHA())
                    // check if need trigger databricks CI build
                    if (title ==~ /.*\[databricks\].*/ || databricksCodeChanged()) {
                        db_build = true
                    }
                }
            }
        } // end of Init githubHelper

        stage('Build docker image') {
            when {
                beforeAgent true
                expression {
                    !skipped
                }
            }

            agent {
                kubernetes {
                    label "premerge-docker-${BUILD_TAG}"
                    cloud 'sc-ipp-blossom-prod'
                    yaml pod.getDockerBuildYAML()
                    workspaceVolume persistentVolumeClaimWorkspaceVolume(claimName: "${PVC}", readOnly: false)
                    customWorkspace "${CUSTOM_WORKSPACE}"
                }
            }

            steps {
                script {
                    githubHelper.updateCommitStatus("$BUILD_URL", "Running", GitHubCommitState.PENDING)
                    unstash "source_tree"
                    container('docker-build') {
                        // check if pre-merge dockerfile modified
                        def dockerfileModified = sh(returnStdout: true,
                                script: 'BASE=$(git --no-pager log --oneline -1 | awk \'{ print $NF }\'); ' +
                                        'git --no-pager diff --name-only HEAD $(git merge-base HEAD $BASE) ' +
                                        "-- ${PREMERGE_DOCKERFILE} || true")
                        if (!dockerfileModified?.trim()) {
                            TEMP_IMAGE_BUILD = false
                        }

                        if (TEMP_IMAGE_BUILD) {
                            IMAGE_TAG = "dev-ubuntu18-${CUDA_NAME}"
                            PREMERGE_TAG = "${IMAGE_TAG}-${BUILD_TAG}"
                            IMAGE_PREMERGE = "${ARTIFACTORY_NAME}/sw-spark-docker-local/plugin:${PREMERGE_TAG}"
                            def CUDA_VER = "$CUDA_NAME" - "cuda"
                            docker.build(IMAGE_PREMERGE, "-f ${PREMERGE_DOCKERFILE} --build-arg CUDA_VER=$CUDA_VER -t $IMAGE_PREMERGE .")
                            uploadDocker(IMAGE_PREMERGE)
                        } else {
                            // if no pre-merge dockerfile change, use nightly image
                            IMAGE_PREMERGE = "$ARTIFACTORY_NAME/sw-spark-docker-local/plugin:dev-ubuntu18-$CUDA_NAME-blossom-dev"
                        }
                    }
                }
            }
        } // end of Build docker image

        stage('Init DB') {
            when {
                beforeAgent true
                expression {
                    db_build
                }
            }
            agent {
                kubernetes {
                    label "premerge-ci-db-init-${BUILD_NUMBER}"
                    cloud 'sc-ipp-blossom-prod'
                    yaml "${IMAGE_DB}"
                }
            }
            steps {
                script {
                    githubHelper.updateCommitStatus("$BUILD_URL", "Running - includes databricks", GitHubCommitState.PENDING)
                    unstash "source_tree"
                    container('cpu') {
                        sh """
                            bash -c 'dbfs mkdirs $INIT_SCRIPTS_DIR'
                            bash -c 'dbfs cp --overwrite jenkins/databricks/init_cudf_udf.sh $INIT_SCRIPTS_DIR'
                            bash -c 'dbfs cp --overwrite jenkins/databricks/init_cuda11_runtime.sh $INIT_SCRIPTS_DIR'
                        """
                    }
                }
            }
        }

        stage('Premerge Test') {
            when {
                beforeAgent true
                beforeOptions true
                expression {
                    !skipped
                }
            }
            // Parallel run mvn verify (build and integration tests) and unit tests (for multiple Spark versions)
            // If any one is failed will abort another if not finish yet and will upload failure log to Github
            failFast true
            parallel {
                stage('mvn verify') {
                    options {
                        // We have to use params to pass the resource label in options block,
                        // this is a limitation of declarative pipeline. And we need to lock resource before agent start
                        lock(label: "${params.GPU_POOL}", quantity: 1, variable: 'GPU_RESOURCE')
                    }
                    agent {
                        kubernetes {
                            label "premerge-ci-1-${BUILD_TAG}"
                            cloud 'sc-ipp-blossom-prod'
                            yaml pod.getGPUYAML("${IMAGE_PREMERGE}", "${env.GPU_RESOURCE}", '8', '32Gi')
                            workspaceVolume persistentVolumeClaimWorkspaceVolume(claimName: "${PVC}", readOnly: false)
                            customWorkspace "${CUSTOM_WORKSPACE}"
                        }
                    }

                    steps {
                        script {
                            container('gpu') {
                                timeout(time: 4, unit: 'HOURS') { // step only timeout for test run
                                    try {
                                        sh "$PREMERGE_SCRIPT mvn_verify"
                                        step([$class                : 'JacocoPublisher',
                                              execPattern           : '**/target/jacoco.exec',
                                              classPattern          : 'target/jacoco_classes/',
                                              sourceInclusionPattern: '**/*.java,**/*.scala',
                                              sourcePattern         : sourcePattern
                                        ])
                                    } finally {
                                        common.publishPytestResult(this, "${STAGE_NAME}")
                                    }
                                }
                            }
                        }
                    }
                } // end of mvn verify stage

                stage('Premerge CI 2') {
                    options {
                        lock(label: "${params.GPU_POOL}", quantity: 1, variable: 'GPU_RESOURCE')
                    }
                    agent {
                        kubernetes {
                            label "premerge-ci-2-${BUILD_TAG}"
                            cloud 'sc-ipp-blossom-prod'
                            yaml pod.getGPUYAML("${IMAGE_PREMERGE}", "${env.GPU_RESOURCE}", '8', '32Gi')
                            workspaceVolume persistentVolumeClaimWorkspaceVolume(claimName: "${PVC}", readOnly: false)
                            customWorkspace "${CUSTOM_WORKSPACE}-ci-2" // Use different workspace to avoid conflict with IT
                        }
                    }

                    steps {
                        script {
                            unstash "source_tree"
                            container('gpu') {
                                timeout(time: 4, unit: 'HOURS') {
                                    try {
                                        sh "$PREMERGE_SCRIPT ci_2"
                                    } finally {
                                        common.publishPytestResult(this, "${STAGE_NAME}")
                                    }
                                }
                            }
                        }
                    }
                } // end of Unit Test stage

                stage('DB runtime 9.1') {
                    when {
                        beforeAgent true
                        anyOf {
                            expression { db_build }
                        }
                    }

                    agent {
                        kubernetes {
                            label "premerge-ci-db-9.1-${BUILD_NUMBER}"
                            cloud 'sc-ipp-blossom-prod'
                            yaml "${IMAGE_DB}"
                        }
                    }
                    environment {
                        DB_RUNTIME = '9.1'
                        DATABRICKS_RUNTIME = "${RUNTIME_MAP["$DB_RUNTIME"][ID_RUNTIME]}"
                        BASE_SPARK_VERSION = "${RUNTIME_MAP["$DB_RUNTIME"][ID_SPARK]}"
                        BASE_SPARK_VERSION_TO_INSTALL_DATABRICKS_JARS = "${RUNTIME_MAP["$DB_RUNTIME"][ID_INSTALL]}"
                        INIT_SCRIPTS = getInitScripts("$INIT_SCRIPTS_DIR",
                                "${RUNTIME_MAP["$DB_RUNTIME"][ID_INITSCRIPTS]}")
                    }
                    steps {
                        script {
                            timeout(time: 5, unit: 'HOURS') {
                                unstash "source_tree"
                                databricksBuild()
                            }
                        }
                    }
                } // end of DB runtime 9.1

                stage('DB runtime 10.4') {
                    when {
                        beforeAgent true
                        anyOf {
                            expression { db_build }
                        }
                    }

                    agent {
                        kubernetes {
                            label "premerge-ci-db-10.4-${BUILD_NUMBER}"
                            cloud 'sc-ipp-blossom-prod'
                            yaml "${IMAGE_DB}"
                        }
                    }
                    environment {
                        DB_RUNTIME = '10.4'
                        DATABRICKS_RUNTIME = "${RUNTIME_MAP["$DB_RUNTIME"][ID_RUNTIME]}"
                        BASE_SPARK_VERSION = "${RUNTIME_MAP["$DB_RUNTIME"][ID_SPARK]}"
                        BASE_SPARK_VERSION_TO_INSTALL_DATABRICKS_JARS = "${RUNTIME_MAP["$DB_RUNTIME"][ID_INSTALL]}"
                        INIT_SCRIPTS = getInitScripts("$INIT_SCRIPTS_DIR",
                                "${RUNTIME_MAP["$DB_RUNTIME"][ID_INITSCRIPTS]}")
                    }
                    steps {
                        script {
                            timeout(time: 5, unit: 'HOURS') {
                                unstash "source_tree"
                                databricksBuild()
                            }
                        }
                    }
                } // end of DB runtime 10.4

                stage('DB runtime 11.3') {
                    when {
                        beforeAgent true
                        anyOf {
                            expression { db_build }
                        }
                    }

                    agent {
                        kubernetes {
                            label "premerge-ci-db-11.3-${BUILD_NUMBER}"
                            cloud 'sc-ipp-blossom-prod'
                            yaml "${IMAGE_DB}"
                        }
                    }
                    environment {
                        DB_RUNTIME = '11.3'
                        DATABRICKS_RUNTIME = "${RUNTIME_MAP["$DB_RUNTIME"][ID_RUNTIME]}"
                        BASE_SPARK_VERSION = "${RUNTIME_MAP["$DB_RUNTIME"][ID_SPARK]}"
                        BASE_SPARK_VERSION_TO_INSTALL_DATABRICKS_JARS = "${RUNTIME_MAP["$DB_RUNTIME"][ID_INSTALL]}"
                        INIT_SCRIPTS = getInitScripts("$INIT_SCRIPTS_DIR",
                                "${RUNTIME_MAP["$DB_RUNTIME"][ID_INITSCRIPTS]}")
                    }
                    steps {
                        script {
                            timeout(time: 5, unit: 'HOURS') {
                                unstash "source_tree"
                                databricksBuild()
                            }
                        }
                    }
                } // end of DB runtime 11.3

                stage('Dummy stage: blue ocean log view') {
                    steps {
                        echo "workaround for blue ocean bug https://issues.jenkins.io/browse/JENKINS-48879"
                    }
                } // Dummy stage
            } // end of parallel
        } // end of Premerge Test
    } // end of stages

    post {
        always {
            script {
                if (skipped) {
                    return
                }

                if (currentBuild.currentResult == "SUCCESS") {
                    githubHelper.updateCommitStatus("$BUILD_URL", "Success", GitHubCommitState.SUCCESS)
                } else {
                    // upload log only in case of build failure
                    def guardWords = ["gitlab.*?\\.com", "urm.*?\\.com",
                                      "dbc.*?azuredatabricks\\.net", "adb.*?databricks\\.com"]
                    guardWords.add("nvidia-smi(?s)(.*?)(?=jenkins/version-def.sh)") // hide GPU info
                    githubHelper.uploadParallelLogs(this, env.JOB_NAME, env.BUILD_NUMBER, null, guardWords)

                    if (currentBuild.currentResult != "ABORTED") { // skip ABORTED result to avoid status overwrite
                        githubHelper.updateCommitStatus("$BUILD_URL", "Fail", GitHubCommitState.FAILURE)
                    }
                }

                if (db_build) {
                    container('cpu') {
                        sh "bash -c 'dbfs rm -r $INIT_SCRIPTS_DIR || true'"
                    }
                }

                if (TEMP_IMAGE_BUILD) {
                    container('cpu') {
                        deleteDockerTempTag("${PREMERGE_TAG}") // clean premerge temp image
                    }
                }
            }
        }
    }

} // end of pipeline

// params.DATABRICKS_TYPE: 'aws' or 'azure', param can be defined through the jenkins webUI
String getDbType() {
    return params.DATABRICKS_TYPE ? params.DATABRICKS_TYPE : 'aws'
}

// e.g. foo.sh,bar.sh --> /dbfs/path/foo.sh,/dbfs/path/bar.sh
String getInitScripts(String rootDir, String files) {
    return rootDir + '/' + files.replace(',', ',' + rootDir + '/')
}

void databricksBuild() {
    def CLUSTER_ID = ''
    def SPARK_MAJOR = BASE_SPARK_VERSION_TO_INSTALL_DATABRICKS_JARS.replace('.', '')
    try {
        stage("Create $SPARK_MAJOR DB") {
            script {
                container('cpu') {
                    sh "rm -rf spark-rapids-ci.tgz"
                    sh "tar -zcf spark-rapids-ci.tgz *"
                    def CREATE_PARAMS = " -r $DATABRICKS_RUNTIME -w $DATABRICKS_HOST -t $DATABRICKS_TOKEN" +
                            " -s $DB_TYPE -n CI-${BUILD_TAG}-${BASE_SPARK_VERSION} -k \"$DATABRICKS_PUBKEY\" -i $IDLE_TIMEOUT" +
                            " -d $DATABRICKS_DRIVER -o $DATABRICKS_WORKER -e $NUM_WORKERS -f $INIT_SCRIPTS"
                    CLUSTER_ID = sh(script: "python3 ./jenkins/databricks/create.py $CREATE_PARAMS",
                            returnStdout: true).trim()
                    echo CLUSTER_ID
                }
            }
        }

        stage("Build against $SPARK_MAJOR DB") {
            script {
                container('cpu') {
                    withCredentials([file(credentialsId: 'SPARK_DATABRICKS_PRIVKEY', variable: 'DATABRICKS_PRIVKEY')]) {
                        def BUILD_PARAMS = " -w $DATABRICKS_HOST -t $DATABRICKS_TOKEN -c $CLUSTER_ID -z ./spark-rapids-ci.tgz" +
                                " -p $DATABRICKS_PRIVKEY -l ./jenkins/databricks/build.sh -d /home/ubuntu/build.sh" +
                                " -v $BASE_SPARK_VERSION -i $BASE_SPARK_VERSION_TO_INSTALL_DATABRICKS_JARS"
                        sh "python3 ./jenkins/databricks/run-build.py $BUILD_PARAMS"
                    }
                }
            }
        }

        stage("Test agaist $SPARK_MAJOR DB") {
            script {
                // TODO re-enable DB11.3 after all failed tests get fixed
                if (env.DB_RUNTIME != '11.3' ) {
                    container('cpu') {
                        try {
                            withCredentials([file(credentialsId: 'SPARK_DATABRICKS_PRIVKEY', variable: 'DATABRICKS_PRIVKEY')]) {
                                def TEST_PARAMS = " -w $DATABRICKS_HOST -t $DATABRICKS_TOKEN -c $CLUSTER_ID" +
                                    " -p $DATABRICKS_PRIVKEY -l ./jenkins/databricks/test.sh -v $BASE_SPARK_VERSION -d /home/ubuntu/test.sh"
                                if (params.SPARK_CONF) {
                                    TEST_PARAMS += " -f ${params.SPARK_CONF}"
                                }
                                sh "python3 ./jenkins/databricks/run-tests.py $TEST_PARAMS"
                            }
                        } finally {
                            common.publishPytestResult(this, "${STAGE_NAME}")
                        }
                    }
                }
            }
        }

    } finally {
        if (CLUSTER_ID) {
            container('cpu') {
                retry(3) {
                    sh "python3 ./jenkins/databricks/shutdown.py -s $DATABRICKS_HOST -t $DATABRICKS_TOKEN -c $CLUSTER_ID -d"
                }
            }
        }
    }
}

void uploadDocker(String IMAGE_NAME) {
    def DOCKER_CMD = "docker --config $WORKSPACE/.docker"
    retry(3) {
        sleep(time: 10, unit: "SECONDS")
        sh """
            echo $URM_CREDS_PSW | $DOCKER_CMD login $ARTIFACTORY_NAME -u $URM_CREDS_USR --password-stdin
            $DOCKER_CMD push $IMAGE_NAME
            $DOCKER_CMD logout $ARTIFACTORY_NAME
        """
    }
}

void deleteDockerTempTag(String tag) {
    if (!tag?.trim()) { // return if the tag is null or empty
        return
    }
    sh "curl -u $URM_CREDS_USR:$URM_CREDS_PSW -XDELETE https://${ARTIFACTORY_NAME}/artifactory/sw-spark-docker-local/plugin/${tag} || true"
}

void abortDupBuilds() {
    Run prevBuild = currentBuild.rawBuild.getPreviousBuildInProgress()
    while (prevBuild != null) {
        if (prevBuild.isInProgress()) {
            def prevDesc = prevBuild.description?.trim()
            if (prevDesc && prevDesc == currentBuild.description?.trim()) {
                def prevExecutor = prevBuild.getExecutor()
                if (prevExecutor != null) {
                    echo "...Aborting duplicate Build #${prevBuild.number}"
                    prevExecutor.interrupt(Result.ABORTED,
                            new UserInterruption("Build #${currentBuild.number}"))
                }
            }
        }
        prevBuild = prevBuild.getPreviousBuildInProgress()
    }
}

void checkoutCode(String url, String sha) {
    checkout(
        changelog: false,
        poll: true,
        scm: [
            $class           : 'GitSCM', branches: [[name: sha]],
            submoduleCfg     : [],
            userRemoteConfigs: [[
                                    credentialsId: 'github-token',
                                    url          : url,
                                    refspec      : '+refs/pull/*/merge:refs/remotes/origin/pr/*']]
        ]
    )

    stash(name: "source_tree", includes: "**,.git/**", useDefaultExcludes: false)
}

boolean databricksCodeChanged() {
    def output = sh(script: '''
            # get merge BASE from merged pull request. Log message e.g. "Merge HEAD into BASE"
            BASE_REF=$(git --no-pager log --oneline -1 | awk '{ print $NF }')
            git --no-pager diff --name-only ${BASE_REF} HEAD | grep -lE 'sql-plugin/src/main/.*[0-9x-]db/|databricks' || true
        ''', returnStdout: true).trim()

    if (output) {
        echo "Found databricks-related changed files"
        return true
    }
    return false
}
