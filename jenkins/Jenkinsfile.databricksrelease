#!/usr/local/env groovy
/*
 * Copyright (c) 2020, NVIDIA CORPORATION.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
*
* Jenkinsfile for building and deploy rapids-plugin for Databricks to public repo
*
*/
@Library('shared-libs') _

def urmUrl="https://${ArtifactoryConstants.ARTIFACTORY_NAME}/artifactory/sw-spark-maven"

pipeline {
    agent {
        dockerfile {
            label 'docker-deploy||docker-gpu'
            filename 'Dockerfile.ubuntu16'
            dir "jenkins"
            args '--runtime=nvidia -v ${HOME}/.m2:${HOME}/.m2:rw \
                -v ${HOME}/.zinc:${HOME}/.zinc:rw'
        }
    }

    options {
        ansiColor('xterm')
        timeout(time: 120, unit: 'MINUTES')
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }

    parameters {
        string(name: 'DEPLOY_TO', defaultValue: 'https://oss.sonatype.org/service/local/staging/deploy/maven2',
            description: 'The repo URL where to deploy the artifacts')
        string(name: 'DATABRICKS_VERSION',
                defaultValue: '0.2.0-SNAPSHOT', description: 'Version to set')
        string(name: 'CUDF_VERSION',
                defaultValue: '0.15-SNAPSHOT', description: 'Cudf version to use')
        string(name: 'CUDA_VERSION',
                defaultValue: 'cuda10-1', description: 'cuda version to use')
        string(name: 'REF', defaultValue: 'branch-0.2', description: 'Commit to build')
    }

    environment {
        JENKINS_ROOT='jenkins'
        LIBCUDF_KERNEL_CACHE_PATH='/tmp/.cudf'
        MVN_MIRROR='-s jenkins/settings.xml -P mirror-apache-to-urm'
        URM_CREDS = credentials("svcngcc_artifactory")
        DATABRICKS_TOKEN = credentials("SPARK_DATABRICKS_TOKEN")
        DIST_PL='dist'
        SQL_PL='sql-plugin'
        SCALA_VERSION = '2.12'
        SPARK_VERSION = '3.0.0-databricks'
        CI_RAPIDS_JAR = 'rapids-4-spark_2.12-0.1-SNAPSHOT-ci.jar'
        CI_CUDF_JAR = 'cudf-0.14-cuda10-1.jar'
        LOCAL_URL = "${localUrl}"
    }

    stages {
        stage('Build') {
            steps {
                script {
                    sshagent(credentials : ['svcngcc_pubpriv']) {
                        sh "rm spark-rapids-ci.tgz"
                        sh "tar -zcvf spark-rapids-ci.tgz * || true"
                        sh "python3.6 ./jenkins/databricks/run-tests.py -z ./spark-rapids-ci.tgz -t $DATABRICKS_TOKEN -p /home/svcngcc/.ssh/id_rsa -l ./jenkins/databricks/build.sh -j $CI_RAPIDS_JAR -b $DATABRICKS_VERSION -k $SPARK_VERSION -a $SCALA_VERSION -f $CUDF_VERSION -u $CUDA_VERSION -m $CI_CUDF_JAR"
                    }
                }
            }
        }
        stage("Deploy") {
            environment {
                SERVER_ID='ossrh'
                SERVER_URL="${DEPLOY_TO}"
                GPG_PASSPHRASE=credentials('SPARK_RAPIDS_GPG_PASSPHRASE')
                GPG_FILE=credentials('SPARK_RAPIDS_GPG_PRIVATE_KEY')
                SONATYPE=credentials('SPARK_SONATYPE_USERPASS')
                GNUPGHOME="${WORKSPACE}/.gnupg"
            }
            steps {
                script {
                    sh 'rm -rf $GNUPGHOME'
                    sh 'gpg --import $GPG_FILE'
                    retry (3) {
                        sh "bash $JENKINS_ROOT/deploy.sh true true"
                    }
                }
            }
        }
        stage('Cleanup') {
            steps {
                script {
                    sh "python3.6 ./jenkins/databricks/shutdown.py -t $DATABRICKS_TOKEN"
                }
            }
        }
    } // End of stages
} // end of pipeline
