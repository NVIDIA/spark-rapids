#!/usr/local/env groovy
/*
 * Copyright (c) 2020, NVIDIA CORPORATION.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
*
* Jenkinsfile for building rapids-plugin on Databricks based on Spark 3.0.1
*
*/
@Library(['shared-libs', 'spark-jenkins-shared-lib']) _

def urmUrl="https://${ArtifactoryConstants.ARTIFACTORY_NAME}/artifactory/sw-spark-maven"

pipeline {
    agent {
        docker {
            label 'docker-gpu'
            image "${ArtifactoryConstants.ARTIFACTORY_NAME}/sw-spark-docker/plugin:dev-ubuntu16-cuda10.1"
            args '--runtime=nvidia -v ${HOME}/.m2:${HOME}/.m2:rw \
                -v ${HOME}/.zinc:${HOME}/.zinc:rw'
        }
    }

    options {
        ansiColor('xterm')
        // timeout doesn't seem to work with environment variables so make sure to update below
        // IDLE_TIMEOUT config as well
        timeout(time: 240, unit: 'MINUTES')
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }

    parameters {
        choice(name: 'DEPLOY_TO', choices: ['Urm', 'Local'],
            description: 'Where to deploy artifacts to')
        string(name: 'RUNTIME',
                defaultValue: '7.3.x-gpu-ml-scala2.12', description: 'databricks runtime')
        string(name: 'PUBLIC_KEY',
                defaultValue: '\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDB+ValakyoKn7w+iBRoAi1KlLVH4yVmRXhLCZs1qUECBAhbck2o8Lgjp5wJ+epdT3+EAP2+t/zlK1mU9tTylidapR4fZuIk9ApoQSLOUEXcUtHkPpZulIoGAq78HoyiEs1sKovc6ULvpymjdnQ3ogCZlTlP9uqmL2E4kbtrNCNL0SVj/w10AqzrJ5lqQgO5dIdDRMHW2sv88JI1VLlfiSsofa9RdI7hDRuCnfZ0+dv2URJGGzGt2BkdEmk9t5F1BMpuXvZ8HzOYdACzw0U+THBOk9d4CACUYMyO1XqlXwoYweNKRnigXDCRaTWGFBzTkugRuW/BZBccTR1ON430uRB svcngcc@nvidia.com\"', description: 'public key')
        string(name: 'REF', defaultValue: 'branch-0.4', description: 'Commit to build')
        string(name: 'BASE_SPARK_VERSION',
                defaultValue: '3.0.1', description: 'Databricks base Spark version')
        string(name: 'BUILD_PROFILES',
                defaultValue: 'databricks301,!snapshot-shims', description: 'the mvn build profiles to use when building Databricks')
    }

    environment {
        IDLE_TIMEOUT = 240
        JENKINS_ROOT  = 'jenkins'
        MVN_URM_MIRROR='-s jenkins/settings.xml -P mirror-apache-to-urm'
        LIBCUDF_KERNEL_CACHE_PATH='/tmp'
        URM_CREDS = credentials("svcngcc_artifactory")
        DATABRICKS_TOKEN = credentials("SPARK_DATABRICKS_TOKEN")
        URM_URL = "${urmUrl}"
    }

    triggers {
        cron('H 5 * * *')
    }

    stages {
        stage('Ubuntu16 CUDA10.1') {
            steps {
                script {
                    sshagent(credentials : ['svcngcc_pubpriv']) {
                        sh "rm -rf spark-rapids-ci.tgz"
                        sh "tar -zcvf spark-rapids-ci.tgz *"
                        env.CLUSTERID = sh (
                            script: "python3.6 ./jenkins/databricks/create.py -t $DATABRICKS_TOKEN -k $PUBLIC_KEY -r $RUNTIME -i $IDLE_TIMEOUT -n CI-GPU-databricks-${BASE_SPARK_VERSION}",
                            returnStdout: true
                        ).trim()
                        sh "python3.6 ./jenkins/databricks/run-tests.py -c ${env.CLUSTERID} -z ./spark-rapids-ci.tgz -t $DATABRICKS_TOKEN -p /home/svcngcc/.ssh/id_rsa -l ./jenkins/databricks/build.sh -v $BASE_SPARK_VERSION -b $BUILD_PROFILES"
                        sh "./jenkins/databricks/deploy.sh"
                    }
                }
            }
        }
    } // end of stages
    post {
        always {
            script {
                sh "python3.6 ./jenkins/databricks/shutdown.py -c ${env.CLUSTERID} -t $DATABRICKS_TOKEN -d || true"
                if (currentBuild.currentResult == "SUCCESS") {
                    slack("#swrapids-spark-cicd", "Success", color: "#33CC33")
                } else {
                    slack("#swrapids-spark-cicd", "Failed", color: "#FF0000")
                }
            }
        }
    }
} // end of pipeline

void slack(Map params = [:], String channel, String message) {
    Map defaultParams = [
            color: "#000000",
            baseUrl: "${SparkConstants.SLACK_API_ENDPOINT}",
            tokenCredentialId: "slack_token"
    ]

    params["channel"] = channel
    params["message"] = "${BUILD_URL}\n" + message

    slackSend(defaultParams << params)
}
