<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright (c) 2021, NVIDIA CORPORATION.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>com.nvidia</groupId>
        <artifactId>rapids-4-spark-shims_2.12</artifactId>
        <version>21.12.0-SNAPSHOT</version>
        <relativePath>../pom.xml</relativePath>
    </parent>
    <artifactId>rapids-4-spark-shims-spark312db_2.12</artifactId>
    <name>RAPIDS Accelerator for Apache Spark SQL Plugin Databricks 9.1 Shim</name>
    <description>The RAPIDS SQL plugin for Databricks 9.1 Shim</description>
    <version>21.12.0-SNAPSHOT</version>
    <properties>
        <!-- don't move to the parent module shims due to conflicting tmp files -->
        <target.classifier/>
    </properties>

    <!-- Set 'spark.version' for the shims layer -->
    <!-- Create a separate file 'SPARK_VER.properties' in the jar to save cudf & spark version info -->
    <build>
        <plugins>
            <plugin>
                <artifactId>maven-antrun-plugin</artifactId>
                <executions>
                    <execution>
                        <id>dependency</id>
                        <phase>generate-resources</phase>
                        <configuration>
                            <target>
                                <mkdir dir="${project.build.directory}/extra-resources"/>
                                <exec executable="bash" failonerror="true"
                                      output="${project.build.directory}/extra-resources/spark-${spark312db.version}-info.properties">
                                    <arg value="${spark.rapids.source.basedir}/build/dependency-info.sh"/>
                                    <arg value="${cudf.version}"/>
                                    <arg value="${cuda.version}"/>
                                    <arg value="${spark312db.version}"/>
                                </exec>
                            </target>
                        </configuration>
                        <goals>
                            <goal>run</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>

        <resources>
            <resource>
                <!-- Include the properties file to provide the build information. -->
                <directory>${project.build.directory}/extra-resources</directory>
            </resource>
            <resource>
                <directory>src/main/resources</directory>
            </resource>
        </resources>
    </build>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-annotation_${scala.binary.version}</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>

        <!--
             Note that we are using the Spark version for all of the Databricks dependencies as well.
             The jenkins/databricks/build.sh script handles installing the jars as maven artifacts.
             This is to make it easier and not have to change version numbers for each individual dependency
             and deal with differences between Databricks versions
        -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-unsafe_${scala.binary.version}</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-hadoop</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-common</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-column</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-format</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-io</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>com.esotericsoftware.kryo</groupId>
            <artifactId>kryo-shaded-db</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.arrow</groupId>
            <artifactId>arrow-format</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.arrow</groupId>
            <artifactId>arrow-memory</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.arrow</groupId>
            <artifactId>arrow-vector</artifactId>
            <version>${spark312db.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.json4s</groupId>
            <artifactId>JsonAST</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>
    </dependencies>
</project>
