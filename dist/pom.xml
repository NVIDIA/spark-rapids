<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright (c) 2020-2021, NVIDIA CORPORATION.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <parent>
    <groupId>com.nvidia</groupId>
    <artifactId>rapids-4-spark-parent</artifactId>
    <version>21.10.0-SNAPSHOT</version>
  </parent>
  <artifactId>rapids-4-spark_2.12</artifactId>
  <name>RAPIDS Accelerator for Apache Spark Distribution</name>
  <description>Creates the distribution package of the RAPIDS plugin for Apache Spark</description>
  <version>21.10.0-SNAPSHOT</version>

  <profiles>
    <profile>
        <id>default</id>
        <activation>
            <activeByDefault>true</activeByDefault>
        </activation>
        <dependencies>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-sql_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-shuffle_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-udf_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-shims-aggregator_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <!-- required for conf generation script -->
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-sql_${scala.binary.version}</artifactId>
               <scope>provided</scope>
            </dependency>
            <dependency>
              <!-- required for conf generation script -->
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-hive_${scala.binary.version}</artifactId>
              <scope>provided</scope>
            </dependency>
        </dependencies>
  <build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <configuration>
          <artifactSet>
            <excludes>org.slf4j:*</excludes>
          </artifactSet>
	  <transformers>
            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
	  </transformers>
          <shadedArtifactAttached>false</shadedArtifactAttached>
          <createDependencyReducedPom>true</createDependencyReducedPom>
          <relocations>
            <relocation>
              <pattern>org.apache.orc.</pattern>
              <shadedPattern>${rapids.shade.package}.orc.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>org.apache.hadoop.hive.</pattern>
              <shadedPattern>${rapids.shade.package}.hadoop.hive.</shadedPattern>
              <excludes>
                <exclude>org.apache.hadoop.hive.conf.HiveConf</exclude>
                <exclude>org.apache.hadoop.hive.ql.exec.UDF</exclude>
                <exclude>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</exclude>
              </excludes>
            </relocation>
            <relocation>
              <pattern>org.apache.hive.</pattern>
              <shadedPattern>${rapids.shade.package}.hive.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>io.airlift.compress.</pattern>
              <shadedPattern>${rapids.shade.package}.io.airlift.compress.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>org.apache.commons.codec.</pattern>
              <shadedPattern>${rapids.shade.package}.org.apache.commons.codec.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>org.apache.commons.lang.</pattern>
              <shadedPattern>${rapids.shade.package}.org.apache.commons.lang.</shadedPattern>
            </relocation>

            <relocation>
              <pattern>com.google</pattern>
              <shadedPattern>${rapids.shade.package}.com.google</shadedPattern>
            </relocation>
          </relocations>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <filters>
                <filter>
                  <artifact>com.nvidia:rapids-4-spark_2.12</artifact>
                  <includes>
                    <include>META-INF/**</include>
                  </includes>
                  <excludes>
                    <exclude>META-INF/**</exclude>
                    <exclude>META-INF/services/**</exclude>
                  </excludes>
                </filter>
              </filters>
            </configuration>
          </execution>
        </executions>
      </plugin>
  </plugins>
  </build>
    </profile>
    <profile>
        <id>all30X</id>
        <dependencies>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-aggregator_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <!-- required for conf generation script -->
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-sql_${scala.binary.version}</artifactId>
               <scope>provided</scope>
            </dependency>
            <dependency>
              <!-- required for conf generation script -->
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-hive_${scala.binary.version}</artifactId>
              <scope>provided</scope>
            </dependency>
        </dependencies>
  <build>
    <plugins>
        <plugin>
        <!-- wierd but first build the jar for dependencies pick it up,
             it gets overwritten below by ant target
        -->
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-jar-plugin</artifactId>
      </plugin>
      <plugin>
            <artifactId>maven-dependency-plugin</artifactId>
        <executions>
            <execution>
            <id>copy-current</id>
            <phase>generate-sources</phase>
            <goals>
              <goal>copy</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <artifactItems>
            <artifactItem>
              <groupId>com.nvidia</groupId>
              <artifactId>rapids-4-spark-aggregator_2.12</artifactId>
              <classifier>spark301</classifier>
              <version>${project.version}</version>
              <type>jar</type>
              <overWrite>false</overWrite>
              <outputDirectory>${project.build.directory}/deps</outputDirectory>
            </artifactItem>
            <artifactItem>
              <groupId>com.nvidia</groupId>
              <artifactId>rapids-4-spark-aggregator_2.12</artifactId>
              <classifier>spark302</classifier>
              <version>${project.version}</version>
              <type>jar</type>
              <overWrite>false</overWrite>
              <outputDirectory>${project.build.directory}/deps</outputDirectory>
            </artifactItem>
            <artifactItem>
              <groupId>com.nvidia</groupId>
              <artifactId>rapids-4-spark-aggregator_2.12</artifactId>
              <classifier>spark303</classifier>
              <version>${project.version}</version>
              <type>jar</type>
              <overWrite>false</overWrite>
              <outputDirectory>${project.build.directory}/deps</outputDirectory>
            </artifactItem>
          </artifactItems>
          <outputDirectory>${project.build.directory}/deps</outputDirectory>
          <overWriteReleases>false</overWriteReleases>
          <overWriteSnapshots>true</overWriteSnapshots>
        </configuration>
          </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-antrun-plugin</artifactId>
        <!--
             TODO - how to make sure the versions we pull are up to date?
        -->
        <executions>
          <execution>
            <phase>prepare-package</phase>
            <goals><goal>run</goal></goals>
            <id>delete-shim-classes</id>
            <configuration>
              <target>
                <delete
                        dir="${project.build.directory}/parallel-world"
                        includeemptydirs="true"
                />

                <delete>
                  <fileset
                          dir="${project.build.directory}"
                          includes="*.jar"
                  />
                </delete>
              </target>
            </configuration>
          </execution>
          <execution>
            <phase>package</phase>
            <goals><goal>run</goal></goals>
            <id>create-parallel-world</id>
            <configuration>
              <target>

                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark301.jar"
                        dest="${project.build.directory}/parallel-world"
                >
                  <patternset id="sharedWorld">
                    <includesfile name="${project.basedir}/unshimmed-classes.txt"/>
                    <includesfile name="${project.basedir}/unshimmed-extras.txt"/>
                  </patternset>
                </unzip>

<!--                unzip task seems to be broken for $$ files, so using exec with jar -->
<!--
                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark301.jar"
                        dest="${project.build.directory}/parallel-world/"
                >
-->
                  <!--
                  TODO generate automatically based on the number of shims in the build
                  this example works for 3 shims. If we find 3 duplicates of a class then it can
                  be shared
                  find shims -name \*.class -exec md5sum {} + |
                    sort -k 1 |
                    uniq -w32 -c | tr -s ' ' | grep '^ 3 ' | cut -d'/' -f 5- | sort > dist/unshimmed-classes.txt
                  -->
<!--
                  <patternset id="sharedWorld">
                    <excludesfile name="${project.basedir}/shimmed-classes.txt"/>
                    <excludesfile name="${project.basedir}/unshimmed-classes.txt"/>
                    <excludesfile name="${project.basedir}/unshimmed-extras.txt"/>
                  </patternset>
                </unzip>
-->

                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark301.jar"
                    dest="${project.build.directory}/parallel-world/spark301"
                >
                  <patternset id="excludeMeta">
                    <excludesfile name="${project.basedir}/meta-extras.txt"/>
                  </patternset>
                </unzip>
                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark301.jar"
                    dest="${project.build.directory}/parallel-world/"
                >
                  <patternset id="includeMeta">
                    <includesfile name="${project.basedir}/meta-extras.txt"/>
                  </patternset>
                </unzip>
                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark302.jar"
                    dest="${project.build.directory}/parallel-world/spark302"
                >
                    <patternset refid="excludeMeta"/>
                </unzip>
                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark302.jar"
                    dest="${project.build.directory}/parallel-world/"
                >
                    <patternset refid="includeMeta"/>
                </unzip>
                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark303.jar"
                    dest="${project.build.directory}/parallel-world/spark303"
                >
                    <patternset refid="excludeMeta"/>
                </unzip>
                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark303.jar"
                    dest="${project.build.directory}/parallel-world/"
                >
                    <patternset refid="includeMeta"/>
                </unzip>
                  <!-- to keep in sync with sharedWorld: inverse it to exclude -->
                  <!--
                  <patternset id="shimPattern">
                    <includesfile name="${project.basedir}/shimmed-classes.txt"/>
                  </patternset>
-->
                <!-- TODO - add all spark versions -->
                <copy file="com.nvidia.spark.rapids.SparkShimServiceProvider.spark30x" tofile="${project.build.directory}/parallel-world/META-INF/services/com.nvidia.spark.rapids.SparkShimServiceProvider"/>
                <jar
                        destfile="${project.build.directory}/rapids-4-spark_${scala.binary.version}-${project.version}.jar"
                        basedir="${project.build.directory}/parallel-world"
                >
                </jar>
              </target>
            </configuration>
          </execution>
        </executions>
      </plugin>
  </plugins>
  </build>
    </profile>
    <profile>
        <id>individual</id>
        <!--
        <activation>
            <property><name>buildver</name></property>
        </activation>
        -->
        <dependencies>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-aggregator_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <!-- required for conf generation script -->
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-sql_${scala.binary.version}</artifactId>
               <scope>provided</scope>
            </dependency>
            <dependency>
              <!-- required for conf generation script -->
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-hive_${scala.binary.version}</artifactId>
              <scope>provided</scope>
            </dependency>
        </dependencies>
  <build>
    <plugins>
        <plugin>
        <!-- wierd but first build the jar for dependencies pick it up, 
             it gets overwritten below by ant target
        -->
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-jar-plugin</artifactId>
      </plugin>
      <plugin>
            <artifactId>maven-dependency-plugin</artifactId>
        <executions>
            <execution>
            <id>copy-current</id>
            <phase>generate-sources</phase>
            <goals>
              <goal>copy</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <artifactItems>
            <artifactItem>
              <groupId>com.nvidia</groupId>
              <artifactId>rapids-4-spark-aggregator_2.12</artifactId>
              <classifier>${spark.version.classifier}</classifier>
              <version>${project.version}</version>
              <type>jar</type>
              <overWrite>false</overWrite>
              <outputDirectory>${project.build.directory}/deps</outputDirectory>
            </artifactItem>
          </artifactItems>
          <outputDirectory>${project.build.directory}/deps</outputDirectory>
          <overWriteReleases>false</overWriteReleases>
          <overWriteSnapshots>true</overWriteSnapshots>
        </configuration>
          </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-antrun-plugin</artifactId>
        <!--
             TODO - how to make sure the versions we pull are up to date?
        -->
        <executions>
          <execution>
            <phase>prepare-package</phase>
            <goals><goal>run</goal></goals>
            <id>delete-shim-classes</id>
            <configuration>
              <target>
                <delete
                        dir="${project.build.directory}/parallel-world"
                        includeemptydirs="true"
                />

                <delete>
                  <fileset
                          dir="${project.build.directory}"
                          includes="*.jar"
                  />
                </delete>
              </target>
            </configuration>
          </execution>
          <execution>
            <phase>package</phase>
            <goals><goal>run</goal></goals>
            <id>create-parallel-world</id>
            <configuration>
              <target>

                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-${spark.version.classifier}.jar"
                        dest="${project.build.directory}/parallel-world"
                >
                  <patternset id="sharedWorld">
                    <includesfile name="${project.basedir}/unshimmed-classes.txt"/>
                    <includesfile name="${project.basedir}/unshimmed-extras.txt"/>
                  </patternset>
                </unzip>

<!--                unzip task seems to be broken for $$ files, so using exec with jar -->
<!--
                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-spark301.jar"
                        dest="${project.build.directory}/parallel-world/"
                >
-->
                  <!--
                  TODO generate automatically based on the number of shims in the build
                  this example works for 3 shims. If we find 3 duplicates of a class then it can
                  be shared
                  find shims -name \*.class -exec md5sum {} + |
                    sort -k 1 |
                    uniq -w32 -c | tr -s ' ' | grep '^ 3 ' | cut -d'/' -f 5- | sort > dist/unshimmed-classes.txt
                  -->
<!--
                  <patternset id="sharedWorld">
                    <excludesfile name="${project.basedir}/shimmed-classes.txt"/>
                    <excludesfile name="${project.basedir}/unshimmed-classes.txt"/>
                    <excludesfile name="${project.basedir}/unshimmed-extras.txt"/>
                  </patternset>
                </unzip>
-->

                <unzip
                    src="${project.build.directory}/deps/rapids-4-spark-aggregator_${scala.binary.version}-${project.version}-${spark.version.classifier}.jar"
                    dest="${project.build.directory}/parallel-world/${spark.version.classifier}"
                >
                  <!-- to keep in sync with sharedWorld: inverse it to exclude -->
                  <!--
                  <patternset id="shimPattern">
                    <includesfile name="${project.basedir}/shimmed-classes.txt"/>
                  </patternset>
-->
                </unzip>
                <!-- TODO - add all spark versions -->
                <jar
                        destfile="${project.build.directory}/rapids-4-spark_${scala.binary.version}-${project.version}.jar"
                        basedir="${project.build.directory}/parallel-world"
                >
                </jar>
              </target>
            </configuration>
          </execution>
        </executions>
      </plugin>
  </plugins>
  </build>
    </profile>
    <profile>
        <id>release311cdh</id>
        <activation>
            <property>
                <name>buildver</name>
                <value>311cdh</value>
            </property>
        </activation>
        <dependencies>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-sql_${scala.binary.version}</artifactId>
                <version>${spark311cdh.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.apache.curator</groupId>
                        <artifactId>curator-recipes</artifactId>
                    </exclusion>
                </exclusions>
                <scope>provided</scope>
            </dependency>
            <dependency>
                <groupId>org.apache.curator</groupId>
                <artifactId>curator-recipes</artifactId>
                <version>4.3.0.7.2.7.0-184</version>
                <scope>provided</scope>
            </dependency>
        </dependencies>
    </profile>
    <profile>
        <id>pre-merge</id>
        <dependencies>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-sql_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-shuffle_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-udf_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <groupId>com.nvidia</groupId>
               <artifactId>rapids-4-spark-shims-aggregator_${scala.binary.version}</artifactId>
               <version>${project.version}</version>
            </dependency>
            <dependency>
               <!-- required for conf generation script -->
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-sql_${scala.binary.version}</artifactId>
               <scope>provided</scope>
            </dependency>
            <dependency>
              <!-- required for conf generation script -->
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-hive_${scala.binary.version}</artifactId>
              <scope>provided</scope>
            </dependency>
        </dependencies>
        <build>
            <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <configuration>
          <artifactSet>
            <excludes>org.slf4j:*</excludes>
          </artifactSet>
	  <transformers>
            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
	  </transformers>
          <shadedArtifactAttached>false</shadedArtifactAttached>
          <createDependencyReducedPom>true</createDependencyReducedPom>
          <relocations>
            <relocation>
              <pattern>org.apache.orc.</pattern>
              <shadedPattern>${rapids.shade.package}.orc.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>org.apache.hadoop.hive.</pattern>
              <shadedPattern>${rapids.shade.package}.hadoop.hive.</shadedPattern>
              <excludes>
                <exclude>org.apache.hadoop.hive.conf.HiveConf</exclude>
                <exclude>org.apache.hadoop.hive.ql.exec.UDF</exclude>
                <exclude>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</exclude>
              </excludes>
            </relocation>
            <relocation>
              <pattern>org.apache.hive.</pattern>
              <shadedPattern>${rapids.shade.package}.hive.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>io.airlift.compress.</pattern>
              <shadedPattern>${rapids.shade.package}.io.airlift.compress.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>org.apache.commons.codec.</pattern>
              <shadedPattern>${rapids.shade.package}.org.apache.commons.codec.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>org.apache.commons.lang.</pattern>
              <shadedPattern>${rapids.shade.package}.org.apache.commons.lang.</shadedPattern>
            </relocation>

            <relocation>
              <pattern>com.google</pattern>
              <shadedPattern>${rapids.shade.package}.com.google</shadedPattern>
            </relocation>
          </relocations>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <filters>
                <filter>
                  <artifact>com.nvidia:rapids-4-spark_2.12</artifact>
                  <includes>
                    <include>META-INF/**</include>
                  </includes>
                  <excludes>
                    <exclude>META-INF/**</exclude>
                    <exclude>META-INF/services/**</exclude>
                  </excludes>
                </filter>
              </filters>
            </configuration>
          </execution>
        </executions>
      </plugin>
                <plugin>
                    <groupId>org.codehaus.mojo</groupId>
                    <artifactId>exec-maven-plugin</artifactId>
                    <executions>
                        <execution>
                            <id>if_modified_files</id>
                            <phase>verify</phase>
                            <goals>
                                <goal>exec</goal>
                            </goals>
                            <configuration>
                                <executable>bash</executable>
                                <commandlineArgs>-c 'export MODIFIED=$(git status --porcelain | grep "^ M"); [[ -z $MODIFIED ]] &amp;&amp; exit 0 || { echo -e "found modified files during mvn verify:\n$MODIFIED"; exit 1;}'</commandlineArgs>
                              </configuration>
                        </execution>
                    </executions>
                </plugin>
            </plugins>
        </build>
    </profile>
  </profiles>


  <build>
    <plugins>
      <plugin>
        <groupId>net.alchim31.maven</groupId>
        <artifactId>scala-maven-plugin</artifactId>
        <executions>
          <execution>
            <id>update_config</id>
            <phase>verify</phase>
            <goals>
              <goal>run</goal>
            </goals>
            <configuration>
              <launchers>
                <launcher>
                  <id>update_rapids_config</id>
                  <mainClass>com.nvidia.spark.rapids.RapidsConf</mainClass>
                  <args>
                    <arg>${project.basedir}/../docs/configs.md</arg>
                  </args>
                </launcher>
              </launchers>
            </configuration>
          </execution>
          <execution>
            <id>update_supported</id>
            <phase>verify</phase>
            <goals>
              <goal>run</goal>
            </goals>
            <configuration>
              <launchers>
                <launcher>
                  <id>update_rapids_support</id>
                  <mainClass>com.nvidia.spark.rapids.SupportedOpsDocs</mainClass>
                  <args>
                    <arg>${project.basedir}/../docs/supported_ops.md</arg>
                  </args>
                </launcher>
              </launchers>
            </configuration>
          </execution>
          <execution>
            <id>update_supported_tools</id>
            <phase>verify</phase>
            <goals>
              <goal>run</goal>
            </goals>
            <configuration>
              <launchers>
                <launcher>
                  <id>update_rapids_support_tools</id>
                  <mainClass>com.nvidia.spark.rapids.SupportedOpsForTools</mainClass>
                  <args>
                      <arg>${project.basedir}/../tools/src/main/resources/supportedDataSource.csv</arg>
                  </args>
                </launcher>
              </launchers>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <groupId>org.apache.rat</groupId>
        <artifactId>apache-rat-plugin</artifactId>
 <configuration>
                    <excludes>
                        <exclude>*.txt</exclude>
                    </excludes>
                </configuration>
      </plugin>
    </plugins>
  </build>

</project>
