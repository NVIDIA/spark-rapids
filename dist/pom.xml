<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright (c) 2020-2022, NVIDIA CORPORATION.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>com.nvidia</groupId>
        <artifactId>rapids-4-spark-parent</artifactId>
        <version>22.04.0-SNAPSHOT</version>
    </parent>
    <artifactId>rapids-4-spark_2.12</artifactId>
    <name>RAPIDS Accelerator for Apache Spark Distribution</name>
    <description>Creates the distribution package of the RAPIDS plugin for Apache Spark</description>
    <version>22.04.0-SNAPSHOT</version>
    <dependencies>
        <dependency>
            <groupId>com.nvidia</groupId>
            <artifactId>rapids-4-spark-aggregator_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <classifier>${spark.version.classifier}</classifier>
            <scope>compile</scope>
        </dependency>

        <!--
            manually promoting provided cudf as a direct dependency
        -->
        <dependency>
            <groupId>ai.rapids</groupId>
            <artifactId>cudf</artifactId>
            <version>${cudf.version}</version>
            <classifier>${cuda.version}</classifier>
            <scope>compile</scope>
        </dependency>
    </dependencies>

    <properties>
        <target.classifier/>
        <noSnapshot.buildvers>
            311,
            311cdh,
            312,
            313,
            320,
            321
        </noSnapshot.buildvers>
        <snapshot.buildvers>
            314,
            322,
            330
        </snapshot.buildvers>
        <databricks.buildvers>
            312db,
            321db
        </databricks.buildvers>
    </properties>
    <profiles>
        <profile>
            <id>noSnapshotsWithDatabricks</id>
            <properties>
                <included_buildvers>
                    ${noSnapshot.buildvers},
                    ${databricks.buildvers}
                </included_buildvers>
            </properties>
        </profile>
        <profile>
            <id>noSnapshots</id>
            <properties>
                <included_buildvers>
                    ${noSnapshot.buildvers}
                </included_buildvers>
            </properties>
        </profile>
        <profile>
            <!--
            https://spark.apache.org/versioning-policy.html
            Spark's semantic versioning [MAJOR].[FEATURE].[MAINTENANCE]

            The plugin support begins with MAJOR=3

            Most problems with the shim support occur when we cross into the next
            FEATURE version and when we deal with distributions

            Maintain a representative sample one of each:
                - 3.x upstream versions
                - a downstream distro
                - a snapshot
            for testing while minimizing the build time
            TODO add 3.3.0-SNAPSHOT
            -->
            <id>minimumFeatureVersionMix</id>
            <properties>
                <included_buildvers>
                    312,
                    320,
                    311cdh
                </included_buildvers>
            </properties>
        </profile>
        <profile>
            <id>snapshots</id>
            <properties>
                <included_buildvers>
                    ${noSnapshot.buildvers},
                    ${snapshot.buildvers}
                </included_buildvers>
            </properties>
        </profile>
        <profile>
            <id>snapshotsWithDatabricks</id>
            <properties>
                <included_buildvers>
                    ${noSnapshot.buildvers},
                    ${snapshot.buildvers},
                    ${databricks.buildvers}
                </included_buildvers>
            </properties>
        </profile>
        <profile>
            <id>release311cdh</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>311cdh</value>
                </property>
            </activation>
            <properties>
                <included_buildvers>${buildver}</included_buildvers>
            </properties>
            <dependencies>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-sql_${scala.binary.version}</artifactId>
                    <version>${spark311cdh.version}</version>
                    <exclusions>
                        <exclusion>
                            <groupId>org.apache.curator</groupId>
                            <artifactId>curator-recipes</artifactId>
                        </exclusion>
                    </exclusions>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.curator</groupId>
                    <artifactId>curator-recipes</artifactId>
                    <version>4.3.0.7.2.7.0-184</version>
                    <scope>provided</scope>
                </dependency>
            </dependencies>
            <pluginRepositories>
                <pluginRepository>
                    <id>cloudera-repo</id>
                    <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>
                </pluginRepository>
            </pluginRepositories>
        </profile>
        <profile>
            <id>individual</id>
            <activation>
                <activeByDefault>true</activeByDefault>
            </activation>
            <properties>
                <included_buildvers>${buildver}</included_buildvers>
            </properties>
        </profile>
        <profile>
            <id>pre-merge</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.codehaus.mojo</groupId>
                        <artifactId>exec-maven-plugin</artifactId>
                        <executions>
                            <execution>
                                <id>if_modified_files</id>
                                <phase>verify</phase>
                                <goals>
                                    <goal>exec</goal>
                                </goals>
                                <configuration>
                                    <executable>bash</executable>
                                     <commandlineArgs>-c 'export MODIFIED=$(git status --porcelain | grep "^ M"); [[ -z $MODIFIED ]] &amp;&amp; exit 0 || { echo -e "found modified files during mvn verify:\n$MODIFIED"; exit 1;}'</commandlineArgs>
                                </configuration>
                            </execution>
                        </executions>
                    </plugin>
                </plugins>
            </build>
        </profile>
    </profiles>
    <build>
        <resources>
          <resource>
            <directory>${project.build.directory}/extra-resources</directory>
          </resource>
        </resources>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-resources-plugin</artifactId>
                <version>3.2.0</version>
                <executions>
                    <execution>
                        <id>default-resources</id>
                        <phase>process-resources</phase>
                        <configuration>
                            <outputDirectory>${project.build.directory}/parallel-world</outputDirectory>>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <executions>
                    <execution>
                        <id>default-jar</id>
                        <phase>none</phase>
                    </execution>
                    <execution>
                        <id>create-parallel-worlds-jar</id>
                        <phase>package</phase>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                        <configuration>
                            <classesDirectory>${project.build.directory}/parallel-world</classesDirectory>
                            <excludes>
                                <!-- get rid of all maven poms from shim builds -->
                                <exclude>META-INF/maven/**</exclude>
                            </excludes>
                        </configuration>
                    </execution>
                    <execution>
                        <id>default-test-jar</id>
                        <phase>none</phase>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-antrun-plugin</artifactId>
                <executions>
                    <execution>
                        <!-- ensure a phase preceding create-parallel-world and copy-current -->
                        <phase>initialize</phase>
                        <goals>
                            <goal>run</goal>
                        </goals>
                        <id>clean-any-prior-output</id>
                        <configuration>
                            <target>
                                <delete dir="${project.build.directory}/parallel-world"
                                        includeemptydirs="true"/>
                                <delete>
                                    <fileset dir="${project.build.directory}" includes="*.jar"/>
                                </delete>
                                <delete dir="${project.build.directory}/deps" includeemptydirs="true"/>
                                <mkdir dir="${project.build.directory}/deps"/>
                            </target>
                        </configuration>
                    </execution>
                    <execution>
                        <phase>generate-resources</phase>
                        <goals>
                            <goal>run</goal>
                        </goals>
                        <id>create-parallel-world</id>
                        <configuration>
                            <target>
                                <property name="project.basedir" value="${project.basedir}"/>
                                <property name="project.build.directory" value="${project.build.directory}"/>
                                <property name="project.version" value="${project.version}"/>
                                <property name="scala.binary.version" value="${scala.binary.version}"/>
                                <property name="included_buildvers" value="${included_buildvers}"/>
                                <ant
                                    antfile="${project.basedir}/maven-antrun/build-parallel-worlds.xml"
                                    target="build-parallel-worlds"
                                />
                            </target>
                        </configuration>
                    </execution>
                    <execution>
                        <phase>verify</phase>
                        <goals>
                            <goal>run</goal>
                        </goals>
                        <id>reduce-pom-deps-in-the-jar</id>
                        <configuration>
                            <target>
                                <zip update="true" basedir="${project.build.directory}/extra-resources"
                                    destfile="${project.build.directory}/${project.artifactId}-${project.version}.jar"/>
                            </target>
                        </configuration>
                    </execution>
                    <execution>
                        <id>update_config_docs</id>
                        <phase>verify</phase>
                        <goals>
                            <goal>run</goal>
                        </goals>
                        <configuration>
                            <target>
                                <taskdef resource="net/sf/antcontrib/antcontrib.properties"/>
                                <ac:if xmlns:ac="antlib:net.sf.antcontrib">
                                    <equals arg1="spark311" arg2="${spark.version.classifier}"/>
                                    <ac:then>
                                        <java classname="com.nvidia.spark.rapids.RapidsConf" failonerror="true">
                                            <arg value="${project.basedir}/../docs/configs.md"/>
                                        </java>
                                        <java classname="com.nvidia.spark.rapids.SupportedOpsDocs" failonerror="true">
                                            <arg value="${project.basedir}/../docs/supported_ops.md"/>
                                        </java>
                                        <java classname="com.nvidia.spark.rapids.SupportedOpsForTools" failonerror="true">
                                            <arg value="${project.basedir}/../tools/src/main/resources/supportedDataSource.csv"/>
                                        </java>
                                    </ac:then>
                                </ac:if>
                            </target>
                        </configuration>
                    </execution>
                </executions>
                <dependencies>
                    <dependency>
                        <groupId>com.nvidia</groupId>
                        <artifactId>rapids-4-spark-aggregator_${scala.binary.version}</artifactId>
                        <version>${project.version}</version>
                        <classifier>${spark.version.classifier}</classifier>
                    </dependency>
                    <dependency>
                        <groupId>org.apache.spark</groupId>
                        <artifactId>spark-hive_${scala.binary.version}</artifactId>
                        <version>${spark.version}</version>
                        <exclusions>
                            <exclusion>
                                <groupId>org.apache.curator</groupId>
                                <artifactId>curator-recipes</artifactId>
                            </exclusion>
                        </exclusions>
                    </dependency>
                    <dependency>
                        <groupId>org.apache.spark</groupId>
                        <artifactId>spark-avro_${scala.binary.version}</artifactId>
                        <version>${spark.version}</version>
                    </dependency>
                </dependencies>
            </plugin>
            <plugin>
                <groupId>org.apache.rat</groupId>
                <artifactId>apache-rat-plugin</artifactId>
                <configuration>
                    <excludes>
                        <exclude>com.nvidia.spark.rapids.SparkShimServiceProvider.*</exclude>
                        <exclude>dependency-reduced-pom*.xml</exclude>
                        <exclude>*.txt</exclude>
                        <exclude>*.md</exclude>
                    </excludes>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
