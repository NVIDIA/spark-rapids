#!/bin/bash
#
# Copyright (c) 2021-2022, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

set -e

shopt -s extglob

BLOOP_VERSION=${BLOOP_VERSION:-"1.4.13"}
BLOOP_SCALA_VERSION=${BLOOP_SCALA_VERSION:-"2.13"}
SKIP_CLEAN=1

function print_usage() {
  echo "Usage: buildall [OPTION]"
  echo "Options:"
  echo "   -h, --help"
  echo "        print this help message"
  echo "   --debug"
  echo "        enable bash -x tracing right after this option is parsed"
  echo "   --clean"
  echo "        include Maven clean phase"
  echo "   -gb, --generate-bloop"
  echo "        generate projects for Bloop clients: IDE (Scala Metals, IntelliJ) or Bloop CLI"
  echo "   -p=DIST_PROFILE, --profile=DIST_PROFILE"
  echo "        use this profile for the dist module, default: noSnapshots, also supported: snapshots, minimumFeatureVersionMix,"
  echo "        snapshotsWithDatabricks, and noSnapshotsWithDatabricks. NOTE: the Databricks-related spark3XYdb shims"
  echo "        are not built locally, the jars are fetched prebuilt from a remote Maven repo."
  echo "        You can also supply a comma-separated list of build versions. E.g., --profile=320,330 will build only"
  echo "        the distribution jar only for 3.2.0 and 3.3.0"
  echo "   -m=MODULE, --module=MODULE"
  echo "        after finishing parallel builds, resume from dist and build up to and including module MODULE."
  echo "        E.g., --module=integration_tests"
  echo "   -P=N, --parallel=N"
  echo "        Build in parallel, N (4 by default) is passed via -P to xargs"
}

function bloopInstall() {
  BLOOP_DIR="${BLOOP_DIR:-$PWD/.bloop}"
  mkdir -p $BLOOP_DIR
  rm -f $BLOOP_DIR/*

  time (
    for bv in $SPARK_SHIM_VERSIONS; do
      bloop_config_dir="$PWD/.bloop$bv"
      mkdir -p "$bloop_config_dir"
      rm -f "$bloop_config_dir"/*

      mvn install ch.epfl.scala:maven-bloop_${BLOOP_SCALA_VERSION}:${BLOOP_VERSION}:bloopInstall -pl dist -am \
        -Dbloop.configDirectory="$bloop_config_dir" \
        -DdownloadSources=true \
        -Dbuildver="$bv" \
        -DskipTests \
        -Dskip \
        -Dmaven.javadoc.skip \
        -Dmaven.scalastyle.skip=true \
        -Dmaven.updateconfig.skip=true

      specifier="spark$bv"
      for bloop_json in $(echo $bloop_config_dir/*.json); do
        IFS="/" <<< "$bloop_json" read -ra bloop_json_parts
        last_idx=$((${#bloop_json_parts[@]} - 1))
        file="${bloop_json_parts[$last_idx]}"
        project="${file%.json}-$specifier"
        < $bloop_json jq \
          --arg specifier "$specifier" \
          '.project.out=.project.out + "/" + $specifier | .project.name=.project.name + "-" + $specifier' \
           > "$BLOOP_DIR/$project.json"
      done
    done

    echo "Generated Bloop files under $BLOOP_DIR"
  )
}

while [[ "$1" != "" ]] ; do

case "$1" in

--help|-h)
  print_usage
  exit 0
  ;;

--generate-bloop|-gb)
  GEN_BLOOP=true
  ;;

-p=*|--profile=*)
  DIST_PROFILE="${1#*=}"
  ;;

-m=*|--module=*)
  MODULE="${1#*=}"
  ;;

-P=*|--parallel=*)
  BUILD_PARALLEL="${1#*=}"
  ;;

--debug)
  set -x
  ;;

--clean)
  SKIP_CLEAN="0"
  ;;

*)
  echo >&2 "Unknown arg: $1"
  print_usage
  exit 1
  ;;

esac

# advance $1 to the next in the arg list
shift

done

DIST_PROFILE=${DIST_PROFILE:-"noSnapshots"}
[[ "$MODULE" != "" ]] && MODULE_OPT="--projects $MODULE --also-make" || MODULE_OPT=""

case $DIST_PROFILE in

  snapshots?(WithDatabricks))
    SPARK_SHIM_VERSIONS=(
      301
      302
      303
      304
      311
      311cdh
      312
      313
      320
      321
      322
      330
    )
    ;;

  noSnapshots?(WithDatabricks))
    SPARK_SHIM_VERSIONS=(
      301
      302
      303
      311
      311cdh
      312
      320
      321
    )
    ;;

  minimumFeatureVersionMix)
    SPARK_SHIM_VERSIONS=(
      302
      311cdh
      312
      320
    )
    ;;

  3*)
    <<< $DIST_PROFILE IFS="," read -ra SPARK_SHIM_VERSIONS
    INCLUDED_BUILDVERS_OPT="-Dincluded_buildvers=$DIST_PROFILE"
    unset DIST_PROFILE
    ;;

  *)
    echo "ERROR unexpected value for profile: $DIST_PROFILE, see 'buildall --help'"
    exit 1

esac

if [[ "$GEN_BLOOP" == "true" ]]; then
  bloopInstall
  exit 0
fi

[[ "$DIST_PROFILE" != "" ]] && MVN_PROFILE_OPT="-P$DIST_PROFILE" || MVN_PROFILE_OPT=""

# First element in SPARK_SHIM_VERSIONS to do most of the checks
export BASE_VER=${SPARK_SHIM_VERSIONS[0]}
export NUM_SHIMS=${#SPARK_SHIM_VERSIONS[@]}
export BUILD_PARALLEL=${BUILD_PARALLEL:-4}

if [[ "$SKIP_CLEAN" != "1" ]]; then
  echo Clean once across all modules
  mvn -q clean
fi

echo "Building a combined dist jar with Shims for ${SPARK_SHIM_VERSIONS[@]} ..."

export MVN_BASE_DIR=$(mvn help:evaluate -Dexpression=project.basedir -q -DforceStdout)

function build_single_shim() {
  set -x
  BUILD_VER=$1
  mkdir -p "$MVN_BASE_DIR/target"
  (( BUILD_PARALLEL == 1 || NUM_SHIMS == 1 )) && LOG_FILE="/dev/tty" || \
    LOG_FILE="$MVN_BASE_DIR/target/mvn-build-$BUILD_VER.log"

  if [[ "$BUILD_VER" == "$BASE_VER" ]]; then
    SKIP_CHECKS="false"
    # WORKAROUND:
    # maven build on L193 currently relies on aggregator dependency which
    # will removed by
    # https://github.com/NVIDIA/spark-rapids/issues/3932
    #
    # if it were a single maven invocation Maven would register and give
    # precedence package-phase artifacts
    #
    MVN_PHASE="install"
  else
    SKIP_CHECKS="true"
    MVN_PHASE="package"
  fi

  echo "#### REDIRECTING mvn output to $LOG_FILE ####"
  mvn -U "$MVN_PHASE" \
      -DskipTests \
      -Dbuildver="$BUILD_VER" \
      -Drat.skip="$SKIP_CHECKS" \
      -Dmaven.javadoc.skip="$SKIP_CHECKS" \
      -Dskip \
      -Dmaven.scalastyle.skip="$SKIP_CHECKS" \
      -pl aggregator -am > "$LOG_FILE" 2>&1 || {
        [[ "$LOG_FILE" != "/dev/tty" ]] && tail -20 "$LOG_FILE" || true
        exit 255
      }
}
export -f build_single_shim

# Install all the versions for DIST_PROFILE

# First build the aggregator module for all SPARK_SHIM_VERSIONS in parallel skipping expensive plugins that
# - either deferred to 301 because the check is identical in all shim profiles such as scalastyle
# - or deferred to 301 because we currently don't require it per shim such as javadoc generation
# - or there is a dedicated step to run against a particular shim jar such as unit tests, in
#   the near future we will run unit tests against a combined multi-shim jar to catch classloading
#   regressions even before pytest-based integration_tests
#
# Then resume maven build from the dist module now that shims have been installed
time (
  # printf a single buildver array element per line
  printf "%s\n" "${SPARK_SHIM_VERSIONS[@]}" | \
    xargs -t -I% -P "$BUILD_PARALLEL" -n 1 \
    bash -c 'build_single_shim "$@"' _ %
  # This used to resume from dist. However, without including aggregator in the build
  # the build does not properly initialize spark.version property via buildver profiles
  # in the root pom, and we get a missing spark301 dependency even for --profile=312,321
  # where the build does not require it. Moving it to aggregator resolves this issue with
  # a negligible increase of the build time by ~2 seconds.
  joinShimBuildFrom="aggregator"
  echo "Resuming from $joinShimBuildFrom build only using $BASE_VER"
  mvn package -rf $joinShimBuildFrom $MODULE_OPT $MVN_PROFILE_OPT $INCLUDED_BUILDVERS_OPT \
    -Dbuildver="$BASE_VER" \
    -DskipTests -Dskip -Dmaven.javadoc.skip
)
