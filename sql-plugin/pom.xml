<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright (c) 2020-2022, NVIDIA CORPORATION.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>com.nvidia</groupId>
        <artifactId>rapids-4-spark-parent</artifactId>
        <version>22.12.0-SNAPSHOT</version>
    </parent>
    <artifactId>rapids-4-spark-sql_2.12</artifactId>
    <name>RAPIDS Accelerator for Apache Spark SQL Plugin</name>
    <description>The RAPIDS SQL plugin for Apache Spark</description>
    <version>22.12.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>com.nvidia</groupId>
            <artifactId>spark-rapids-jni</artifactId>
            <classifier>${cuda.version}</classifier>
        </dependency>
        <dependency>
            <groupId>com.nvidia</groupId>
            <artifactId>rapids-4-spark-common_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
        </dependency>
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
            <scope>test</scope>
        </dependency>

        <!-- Flat buffers is a small jar, it's appropriate to use a fixed version -->
        <!-- Shade and relocate it in the aggregator module-->
        <dependency>
            <groupId>com.google.flatbuffers</groupId>
            <artifactId>flatbuffers-java</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.iceberg</groupId>
            <artifactId>iceberg-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.iceberg</groupId>
            <artifactId>iceberg-bundled-guava</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.iceberg</groupId>
            <artifactId>iceberg-core</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-avro_${scala.binary.version}</artifactId>
        </dependency>
        <dependency>
            <!-- Used for Alluxio mounting -->
            <groupId>org.alluxio</groupId>
            <artifactId>alluxio-shaded-client</artifactId>
        </dependency>
    </dependencies>

    <profiles>
        <profile>
            <id>release321cdh</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>321cdh</value>
                </property>
            </activation>
            <dependencies>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-sql_${scala.binary.version}</artifactId>
                    <version>${spark321cdh.version}</version>
                    <exclusions>
                        <exclusion>
                            <groupId>org.apache.arrow</groupId>
                            <artifactId>arrow-vector</artifactId>
                        </exclusion>
                    </exclusions>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-hive_${scala.binary.version}</artifactId>
                    <version>${spark321cdh.version}</version>
                    <exclusions>
                        <!-- spark-core tries to pull a curator-recipes version we don't want -->
                        <exclusion>
                            <groupId>org.apache.spark</groupId>
                            <artifactId>spark-core_${scala.binary.version}</artifactId>
                        </exclusion>
                        <exclusion>
                            <groupId>org.apache.arrow</groupId>
                            <artifactId>arrow-vector</artifactId>
                        </exclusion>
                    </exclusions>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.arrow</groupId>
                    <artifactId>arrow-vector</artifactId>
                    <version>${arrow.cdh.version}</version>
                    <scope>provided</scope>
                    <exclusions>
                        <exclusion>
                            <groupId>com.fasterxml.jackson.core</groupId>
                            <artifactId>jackson-core</artifactId>
                        </exclusion>
                        <exclusion>
                            <groupId>com.fasterxml.jackson.core</groupId>
                            <artifactId>jackson-annotations</artifactId>
                        </exclusion>
                        <exclusion>
                            <groupId>io.netty</groupId>
                            <artifactId>netty-common</artifactId>
                        </exclusion>
                    </exclusions>
                </dependency>
            </dependencies>
        </profile>
        <profile>
            <id>release330cdh</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>330cdh</value>
                </property>
            </activation>
            <dependencies>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-sql_${scala.binary.version}</artifactId>
                    <version>${spark330cdh.version}</version>
                    <exclusions>
                        <exclusion>
                            <groupId>org.apache.arrow</groupId>
                            <artifactId>arrow-vector</artifactId>
                        </exclusion>
                    </exclusions>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-hive_${scala.binary.version}</artifactId>
                    <version>${spark330cdh.version}</version>
                    <exclusions>
                        <!-- spark-core tries to pull a curator-recipes version we don't want -->
                        <exclusion>
                            <groupId>org.apache.spark</groupId>
                            <artifactId>spark-core_${scala.binary.version}</artifactId>
                        </exclusion>
                        <exclusion>
                            <groupId>org.apache.arrow</groupId>
                            <artifactId>arrow-vector</artifactId>
                        </exclusion>
                    </exclusions>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.arrow</groupId>
                    <artifactId>arrow-vector</artifactId>
                    <version>${arrow.cdh.version}</version>
                    <scope>provided</scope>
                    <exclusions>
                        <exclusion>
                            <groupId>com.fasterxml.jackson.core</groupId>
                            <artifactId>jackson-core</artifactId>
                        </exclusion>
                        <exclusion>
                            <groupId>com.fasterxml.jackson.core</groupId>
                            <artifactId>jackson-annotations</artifactId>
                        </exclusion>
                        <exclusion>
                            <groupId>io.netty</groupId>
                            <artifactId>netty-common</artifactId>
                        </exclusion>
                    </exclusions>
                </dependency>
            </dependencies>
        </profile>
        <profile>
            <!--
                 Note that we are using the Spark version for all of the Databricks dependencies as well.
                 The jenkins/databricks/build.sh script handles installing the jars as maven artifacts.
                 This is to make it easier and not have to change version numbers for each individual dependency
                 and deal with differences between Databricks versions
            -->
            <id>dbdeps</id>
            <activation>
                <property>
                    <name>databricks</name>
                </property>
            </activation>
            <dependencies>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-sql_${scala.binary.version}</artifactId>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-avro_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
		    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.avro</groupId>
                    <artifactId>avro-mapred</artifactId>
                    <version>${spark.version}</version>
		    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.avro</groupId>
                    <artifactId>avro</artifactId>
                    <version>${spark.version}</version>
		    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-exec</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-serde</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-hive_${scala.binary.version}</artifactId>
                </dependency>
                <dependency>
                    <groupId>com.fasterxml.jackson.core</groupId>
                    <artifactId>jackson-core</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>com.fasterxml.jackson.core</groupId>
                    <artifactId>jackson-annotations</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-core_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-annotation_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-network-common_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-network-shuffle_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-launcher_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-unsafe_${scala.binary.version}</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.json4s</groupId>
                    <artifactId>JsonAST</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.commons</groupId>
                    <artifactId>commons-io</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.scala-lang</groupId>
                    <artifactId>scala-reflect</artifactId>
                    <version>${scala.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.commons</groupId>
                    <artifactId>commons-lang3</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>com.esotericsoftware.kryo</groupId>
                    <artifactId>kryo-shaded-db</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.parquet</groupId>
                    <artifactId>parquet-hadoop</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.parquet</groupId>
                    <artifactId>parquet-common</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.parquet</groupId>
                    <artifactId>parquet-column</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.parquet</groupId>
                    <artifactId>parquet-format</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.arrow</groupId>
                    <artifactId>arrow-memory</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.arrow</groupId>
                    <artifactId>arrow-vector</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-client</artifactId>
                    <version>${hadoop.client.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.orc</groupId>
                    <artifactId>orc-core</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.orc</groupId>
                    <artifactId>orc-shims</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.orc</groupId>
                    <artifactId>orc-mapreduce</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-storage-api</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
                <dependency>
                    <groupId>com.google.protobuf</groupId>
                    <artifactId>protobuf-java</artifactId>
                    <version>${spark.version}</version>
                    <scope>provided</scope>
                </dependency>
            </dependencies>
        </profile>

        <profile>
            <id>with-classifier</id>
            <activation>
                <activeByDefault>true</activeByDefault>
            </activation>
            <dependencies>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-hive_${scala.binary.version}</artifactId>
                </dependency>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-sql_${scala.binary.version}</artifactId>
                </dependency>
            </dependencies>
        </profile>

    </profiles>

    <build>
        <resources>
          <resource>
            <!-- Include the properties file to provide the build information. -->
            <directory>${project.build.directory}/extra-resources</directory>
            <filtering>true</filtering>
          </resource>
          <resource>
            <directory>${project.basedir}/..</directory>
            <targetPath>META-INF</targetPath>
            <includes>
              <!-- The NOTICE will be taken care of by the antrun task below -->
              <include>LICENSE</include>
            </includes>
          </resource>
          <resource>
            <!-- Include python files for Python UDF support. -->
            <directory>${project.basedir}/../python/</directory>
          </resource>
        </resources>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <configuration>
                    <archive>
                       <!-- transient jar, writing compressed can take several x time -->
                        <compress>false</compress>
                    </archive>
                    <classifier>${spark.version.classifier}</classifier>
                </configuration>
            </plugin>
            <plugin>
              <artifactId>maven-antrun-plugin</artifactId>
              <executions>
                <execution>
                    <id>create-source-path-properties</id>
                    <goals><goal>run</goal></goals>
                    <phase>initialize</phase>
                    <configuration>
                        <exportAntProperties>true</exportAntProperties>
                        <target>
                            <!--
                                Rules for adding new shim directories:
                                ######################################
                                1. Keep includes sorted
                                2. Top path components such as 311until320-all should not use
                                wildcards until a robust pattern is developed that allows
                                refactoring for new shims without breaking build for
                                exisiting shims
                                3. Using wild cards to pick up scala and java with a single entry
                                311until320-all/* is allowed
                                4. Using wildcards for trivial excludes is allowed
                                  - *nondb* dirs in db shims
                                  - *until330* in 330
                                5. Begin with a common reusable pattern, and put the unique includeds
                                and excludes at the end
                                6. At the same pattern-nesting level list includes before excludes
                            -->

                            <!-- upstream Spark shims -->
                            <pathconvert property="spark311.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset id="spark311+.pattern">
                                        <include name="311+-nondb/*"/>
                                        <include name="311until320-all/*"/>
                                        <include name="311until320-noncdh/*"/>
                                        <include name="311until320-nondb/*"/>
                                        <include name="311until330-all/*"/>
                                        <include name="311until330-nondb/*"/>
                                        <include name="311until340-all/*"/>
                                        <include name="pre320-treenode/*"/>
                                    </patternset>
                                    <include name="311-nondb/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark312.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset refid="spark311+.pattern"/>
                                    <include name="312-nondb/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark313.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset refid="spark311+.pattern"/>
                                    <include name="313/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark314.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset refid="spark311+.pattern"/>
                                    <include name="314/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark320.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset id="spark320+.pattern">
                                        <!-- inherit from 311+ with exceptions -->
                                        <patternset refid="spark311+.pattern"/>
                                        <exclude name="*until320*/*"/>
                                        <exclude name="pre320-treenode/*"/>

                                        <!-- uniquely 320+ -->
                                        <include name="320+/*"/>
                                        <include name="320+-noncdh/*"/>
                                        <include name="320+-nondb/*"/>
                                        <include name="320until330-all/*"/>
                                        <include name="320until330-noncdh/*"/>
                                        <include name="320until330-nondb/*"/>
                                        <include name="320until340-all/*"/>
                                        <include name="post320-treenode/*"/>
                                    </patternset>
                                    <include name="320/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark321.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset id="spark321+.pattern">
                                        <patternset refid="spark320+.pattern"/>
                                        <include name="321+/*"/>
                                        <include name="321until330-all/*"/>
                                    </patternset>
                                    <include name="321/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark322.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset refid="spark321+.pattern"/>
                                    <include name="322/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark330.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset id="spark330+.pattern">
                                        <!-- inherit from 321+ with exceptions -->
                                        <patternset refid="spark321+.pattern"/>
                                        <exclude name="*until330*/*"/>

                                        <!-- uniquely 330+ -->
                                        <include name="330+/*"/>
                                        <include name="330+-noncdh/*"/>
                                        <include name="330until340/*"/>
                                    </patternset>
                                    <include name="330/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark331.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset id="spark331+.pattern">
                                        <patternset refid="spark330+.pattern"/>
                                        <include name="331+/*"/>
                                    </patternset>
                                    <include name="331/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark332.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset refid="spark331+.pattern"/>
                                    <include name="332/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark340.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset id="spark340+.pattern">
                                        <!-- inherit from 331+ with exceptions -->
                                        <patternset refid="spark331+.pattern"/>
                                        <exclude name="*until340*/*"/>

                                        <!-- uniquely 340+ -->
                                        <include name="340+/*"/>
                                    </patternset>
                                    <include name="340/*"/>
                                </dirset>
                            </pathconvert>

                            <!-- Spark vendor shims -->
                            <pathconvert property="spark321cdh.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <!-- inherit from 321+ upstream except noncdh -->
                                    <patternset refid="spark321+.pattern"/>
                                    <exclude name="*noncdh*/*"/>

                                    <include name="321cdh/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark330cdh.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <!-- inherit from 330+ upstream except noncdh -->
                                    <patternset refid="spark330+.pattern"/>
                                    <exclude name="*noncdh*/*"/>

                                    <include name="330cdh/*"/>
                                </dirset>
                            </pathconvert>

                            <pathconvert property="spark312db.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <!-- inherit 311+ except nondb -->
                                    <patternset refid="spark311+.pattern"/>
                                    <exclude name="pre320-treenode/*"/>
                                    <exclude name="*nondb*/*"/>

                                    <include name="311+-db/*"/>
                                    <include name="31xdb/*"/>
                                    <include name="post320-treenode/*"/>

                                    <include name="312db/*"/>
                                </dirset>
                            </pathconvert>
                            <pathconvert property="spark321db.sources" pathsep=",">
                                <dirset dir="${project.basedir}/src/main">
                                    <patternset id="spark321db+.pattern">
                                        <!-- inherit 321+ except nondb -->
                                        <patternset refid="spark321+.pattern"/>
                                        <exclude name="*nondb*/*"/>

                                        <include name="311+-db/*"/>
                                    </patternset>
                                    <include name="321db/*"/>
                                </dirset>
                            </pathconvert>
                        </target>
                    </configuration>
                </execution>
                <execution>
                  <id>copy-notice</id>
                  <goals>
                    <goal>run</goal>
                  </goals>
                  <phase>process-resources</phase>
                  <configuration>
                    <target>
                      <!-- copy NOTICE-binary to NOTICE -->
                      <copy
                          todir="${project.build.directory}/classes/META-INF/"
                          verbose="true">
                        <fileset dir="${project.basedir}/..">
                          <include name="NOTICE-binary"/>
                        </fileset>
                        <mapper type="glob" from="*-binary" to="*"/>
                      </copy>
                    </target>
                  </configuration>
                </execution>
                <execution>
                  <id>generate-shim-service</id>
                  <phase>generate-resources</phase>
                  <goals>
                      <goal>run</goal>
                  </goals>
                  <configuration>
                      <target>
                          <property name="servicesDir"
                                    value="${project.build.directory}/extra-resources/META-INF/services"/>
                          <mkdir dir="${servicesDir}"/>
                          <echo file="${servicesDir}/com.nvidia.spark.rapids.SparkShimServiceProvider"
                                message="com.nvidia.spark.rapids.shims.${spark.version.classifier}.SparkShimServiceProvider${line.separator}"/>
                      </target>
                  </configuration>
                </execution>
              </executions>
            </plugin>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
            </plugin>
            <plugin>
                <groupId>org.apache.rat</groupId>
                <artifactId>apache-rat-plugin</artifactId>
            </plugin>
            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
            </plugin>
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>build-helper-maven-plugin</artifactId>
                <executions>
                    <execution>
                        <id>add-shim-sources</id>
                        <phase>generate-sources</phase>
                        <goals><goal>add-source</goal></goals>
                        <configuration>
                            <sources>${spark.shim.sources}</sources>
                        </configuration>
                    </execution>
                    <execution>
                        <id>add-shim-test-sources</id>
                        <phase>generate-test-sources</phase>
                        <goals><goal>add-test-source</goal></goals>
                        <configuration>
                            <sources>
                                <source>${spark.shim.test.sources}</source>
                            </sources>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
