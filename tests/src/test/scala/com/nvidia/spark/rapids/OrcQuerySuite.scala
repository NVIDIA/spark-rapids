/*
 * Copyright (c) 2023, NVIDIA CORPORATION.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.nvidia.spark.rapids

import java.io.File

import scala.collection.mutable.ListBuffer

import com.nvidia.spark.rapids.Arm.withResourceIfAllowed
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileUtil.fullyDelete
import org.apache.hadoop.fs.Path
import org.apache.orc.{OrcFile, Reader}
import org.apache.spark.{SparkConf, SparkContext}

import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.rapids.{MyDenseVector, MyDenseVectorUDT}
import org.apache.spark.sql.types._

/**
 * This corresponds to the Spark class:
 * org.apache.spark.sql.execution.datasources.orc.OrcQueryTest
 */
class OrcQuerySuite extends SparkQueryCompareTestSuite {

  private def getSchema: StructType = new StructType(Array(
    StructField("c0", DataTypes.IntegerType),
    StructField("c1", new MyDenseVectorUDT)
  ))

  private def getData: Seq[Row] = Seq(Row(1, new MyDenseVector(Array(0.25, 2.25, 4.25))))

  private def getDf(spark: SparkSession): DataFrame = {
    spark.createDataFrame(
      SparkContext.getOrCreate().parallelize(getData, numSlices = 1),
      getSchema)
  }

  Seq("orc", "").foreach { v1List =>
    val sparkConf = new SparkConf().set("spark.sql.sources.useV1SourceList", v1List)
    testGpuWriteFallback(
      "Writing User Defined Type(UDT) to ORC fall back, source list is (" + v1List + ")",
      "DataWritingCommandExec",
      spark => getDf(spark),
      // WriteFilesExec is a new operator from Spark version 340, for simplicity, add it here for
      // all Spark versions.
      execsAllowedNonGpu = Seq("DataWritingCommandExec", "WriteFilesExec", "ShuffleExchangeExec"),
      conf = sparkConf
    ) { frame =>
      val tempFile = File.createTempFile("orc-test-udt-write", ".orc")
      try {
        frame.write.mode("overwrite").orc(tempFile.getAbsolutePath)
      } finally {
        fullyDelete(tempFile)
      }
    }
  }

  /**
   * udt.orc is generated by CPU, the schema and the data are from `getSchema` and `getData`.
   * udt.orc meta is: struct<c0:int,c1:array<double>>,
   * The MyDenseVectorUDT type is converted to array<double> in the ORC file.
   * Gpu can read this ORC file when not specifying the schema
   */
  testSparkResultsAreEqual("Reading User Defined Type(UDT) from ORC, not specify schema",
    spark => {
      val path = TestResourceFinder.getResourcePath("udt.orc")
      // not specify schema
      spark.read.orc(path)
    }
  ) {
    frame => frame
  }

  /**
   * udt.orc is generated by CPU, the schema and the data are from `getSchema` and `getData`.
   * udt.orc meta is: struct<c0:int,c1:array<double>>,
   * The MyDenseVectorUDT type is converted to array<double> in the ORC file.
   */
  testGpuFallback("Reading User Defined Type(UDT) from ORC falls back when specify schema",
    "FileSourceScanExec",
    spark => {
      val path = TestResourceFinder.getResourcePath("udt.orc")
      // specify schema
      spark.read.schema(getSchema).orc(path)
    },
    execsAllowedNonGpu = Seq("FileSourceScanExec", "ShuffleExchangeExec")
  ) {
    frame => frame
  }

  private def getOrcFilePostfix(compression: String): String =
    if (Seq("NONE", "UNCOMPRESSED").contains(compression)) {
      ".orc"
    } else {
      s".${compression.toLowerCase()}.orc"
    }

  def checkCompressType(compression: Option[String], orcCompress: Option[String]): Unit = {
    withGpuSparkSession { spark =>
      withTempPath { file =>
        var writer = spark.range(0, 10).write
        writer = compression.map(t => writer.option("compression", t)).getOrElse(writer)
        writer = orcCompress.map(t => writer.option("orc.compress", t)).getOrElse(writer)
        // write ORC file on GPU
        writer.orc(file.getCanonicalPath)

        // expectedType: first use compression, then orc.compress
        var expectedType = compression.getOrElse(orcCompress.get)
        // ORC use NONE for UNCOMPRESSED
        if (expectedType == "UNCOMPRESSED") expectedType = "NONE"
        val maybeOrcFile = file.listFiles()
            .find(_.getName.endsWith(getOrcFilePostfix(expectedType)))
        assert(maybeOrcFile.isDefined)

        // check the compress type using ORC jar
        val orcFilePath = new Path(maybeOrcFile.get.getAbsolutePath)
        val conf = OrcFile.readerOptions(new Configuration())

        // the reader is not a AutoCloseable for Spark CDH, so use `withResourceIfAllowed`
        // 321cdh uses lower ORC: orc-core-1.5.1.7.1.7.1000-141.jar
        // 330cdh uses lower ORC: orc-core-1.5.1.7.1.8.0-801.jar
        withResourceIfAllowed(OrcFile.createReader(orcFilePath, conf)) { reader =>
          // check
          assert(expectedType === reader.getCompressionKind.name)
        }
      }
    }
  }

  private val supportedWriteCompressTypes = {
    // GPU ORC writing does not support ZLIB, LZ4, refer to GpuOrcFileFormat
    val supportedWriteCompressType = ListBuffer("UNCOMPRESSED", "NONE", "ZSTD", "SNAPPY")
    // Cdh321, Cdh330 does not support ZSTD, refer to the Cdh Class:
    // org.apache.spark.sql.execution.datasources.orc.OrcOptions
    // Spark 31x do not support lz4, zstd
    if (isCdh321 || isCdh330 || !VersionUtils.isSpark320OrLater) {
      supportedWriteCompressType -= "ZSTD"
    }
    supportedWriteCompressType
  }

  test("SPARK-16610: Respect orc.compress (i.e., OrcConf.COMPRESS) when compression is unset") {
    // Respect `orc.compress` (i.e., OrcConf.COMPRESS).
    supportedWriteCompressTypes.foreach { orcCompress =>
      checkCompressType(None, Some(orcCompress))
    }

    // make paris, e.g.: [("UNCOMPRESSED", "NONE"), ("NONE", "SNAPPY"), ("SNAPPY", "ZSTD") ... ]
    val pairs = supportedWriteCompressTypes.sliding(2).toList.map(pair => (pair.head, pair.last))

    // "compression" overwrite "orc.compress"
    pairs.foreach { case (compression, orcCompress) =>
      checkCompressType(Some(compression), Some(orcCompress))
    }
  }

  test("Compression options for writing to an ORC file (SNAPPY, ZLIB and NONE)") {
    supportedWriteCompressTypes.foreach { compression =>
      checkCompressType(Some(compression), None)
    }
  }
}
