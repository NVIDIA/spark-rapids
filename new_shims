Compiled from "SparkShims.scala"
public interface com.nvidia.spark.rapids.SparkShims {
  public abstract java.lang.String parquetRebaseReadKey();
  public abstract java.lang.String parquetRebaseWriteKey();
  public abstract java.lang.String avroRebaseReadKey();
  public abstract java.lang.String avroRebaseWriteKey();
  public abstract java.lang.String parquetRebaseRead(org.apache.spark.sql.internal.SQLConf);
  public abstract java.lang.String parquetRebaseWrite(org.apache.spark.sql.internal.SQLConf);
  public abstract org.apache.spark.sql.execution.command.RunnableCommand v1RepairTableCommand(org.apache.spark.sql.catalyst.TableIdentifier);
  public abstract java.lang.String int96ParquetRebaseRead(org.apache.spark.sql.internal.SQLConf);
  public abstract java.lang.String int96ParquetRebaseWrite(org.apache.spark.sql.internal.SQLConf);
  public abstract java.lang.String int96ParquetRebaseReadKey();
  public abstract java.lang.String int96ParquetRebaseWriteKey();
  public static boolean isCastingStringToNegDecimalScaleSupported$(com.nvidia.spark.rapids.SparkShims);
  public boolean isCastingStringToNegDecimalScaleSupported();
  public abstract org.apache.spark.sql.execution.datasources.parquet.ParquetFilters getParquetFilters(org.apache.parquet.schema.MessageType, boolean, boolean, boolean, boolean, int, boolean, scala.Function1<java.lang.String, java.lang.String>, java.lang.String);
  public abstract boolean isWindowFunctionExec(org.apache.spark.sql.execution.SparkPlan);
  public abstract scala.collection.immutable.Map<java.lang.Class<? extends org.apache.spark.sql.catalyst.expressions.Expression>, com.nvidia.spark.rapids.ExprRule<? extends org.apache.spark.sql.catalyst.expressions.Expression>> getExprs();
  public abstract scala.collection.immutable.Map<java.lang.Class<? extends org.apache.spark.sql.execution.SparkPlan>, com.nvidia.spark.rapids.ExecRule<? extends org.apache.spark.sql.execution.SparkPlan>> getExecs();
  public abstract scala.collection.immutable.Map<java.lang.Class<? extends org.apache.spark.sql.connector.read.Scan>, com.nvidia.spark.rapids.ScanRule<? extends org.apache.spark.sql.connector.read.Scan>> getScans();
  public static scala.collection.immutable.Map getPartitionings$(com.nvidia.spark.rapids.SparkShims);
  public scala.collection.immutable.Map<java.lang.Class<? extends org.apache.spark.sql.catalyst.plans.physical.Partitioning>, com.nvidia.spark.rapids.PartRule<? extends org.apache.spark.sql.catalyst.plans.physical.Partitioning>> getPartitionings();
  public abstract scala.collection.immutable.Map<java.lang.Class<? extends org.apache.spark.sql.execution.command.DataWritingCommand>, com.nvidia.spark.rapids.DataWritingCommandRule<? extends org.apache.spark.sql.execution.command.DataWritingCommand>> getDataWriteCmds();
  public abstract scala.collection.immutable.Map<java.lang.Class<? extends org.apache.spark.sql.execution.command.RunnableCommand>, com.nvidia.spark.rapids.RunnableCommandRule<? extends org.apache.spark.sql.execution.command.RunnableCommand>> getRunnableCmds();
  public abstract org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec newBroadcastQueryStageExec(org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec, org.apache.spark.sql.execution.SparkPlan);
  public abstract org.apache.spark.rdd.RDD<org.apache.spark.sql.catalyst.InternalRow> getFileScanRDD(org.apache.spark.sql.SparkSession, scala.Function1<org.apache.spark.sql.execution.datasources.PartitionedFile, scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow>>, scala.collection.immutable.Seq<org.apache.spark.sql.execution.datasources.FilePartition>, org.apache.spark.sql.types.StructType, scala.collection.immutable.Seq<org.apache.spark.sql.catalyst.expressions.AttributeReference>, scala.Option<org.apache.spark.sql.execution.datasources.FileFormat>);
  public static scala.collection.immutable.Seq getFileScanRDD$default$5$(com.nvidia.spark.rapids.SparkShims);
  public scala.collection.immutable.Seq<org.apache.spark.sql.catalyst.expressions.AttributeReference> getFileScanRDD$default$5();
  public static scala.Option getFileScanRDD$default$6$(com.nvidia.spark.rapids.SparkShims);
  public scala.Option<org.apache.spark.sql.execution.datasources.FileFormat> getFileScanRDD$default$6();
  public abstract boolean shouldFailDivOverflow();
  public abstract scala.PartialFunction<org.apache.spark.sql.execution.SparkPlan, org.apache.spark.sql.execution.exchange.ReusedExchangeExec> reusedExchangeExecPfn();
  public abstract <TreeType extends org.apache.spark.sql.catalyst.trees.TreeNode<?>, A> A attachTreeIfSupported(TreeType, java.lang.String, scala.Function0<A>);
  public static java.lang.String attachTreeIfSupported$default$2$(com.nvidia.spark.rapids.SparkShims);
  public <TreeType extends org.apache.spark.sql.catalyst.trees.TreeNode<?>, A> java.lang.String attachTreeIfSupported$default$2();
  public abstract boolean hasAliasQuoteFix();
  public abstract boolean hasCastFloatTimestampUpcast();
  public abstract scala.collection.immutable.Seq<org.apache.hadoop.fs.FileStatus> filesFromFileIndex(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex);
  public abstract boolean isEmptyRelation(java.lang.Object);
  public abstract java.lang.Object broadcastModeTransform(org.apache.spark.sql.catalyst.plans.physical.BroadcastMode, org.apache.spark.sql.catalyst.InternalRow[]);
  public abstract scala.Option<java.lang.Object> tryTransformIfEmptyRelation(org.apache.spark.sql.catalyst.plans.physical.BroadcastMode);
  public abstract boolean isAqePlan(org.apache.spark.sql.execution.SparkPlan);
  public abstract boolean isExchangeOp(com.nvidia.spark.rapids.SparkPlanMeta<?>);
  public abstract org.apache.spark.sql.catalyst.util.DateFormatter getDateFormatter();
  public abstract org.apache.spark.sql.SparkSession sessionFromPlan(org.apache.spark.sql.execution.SparkPlan);
  public abstract boolean isCustomReaderExec(org.apache.spark.sql.execution.SparkPlan);
  public abstract com.nvidia.spark.rapids.ExecRule<? extends org.apache.spark.sql.execution.SparkPlan> aqeShuffleReaderExec();
  public static boolean isExecutorBroadcastShuffle$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.exchange.ShuffleExchangeLike);
  public boolean isExecutorBroadcastShuffle(org.apache.spark.sql.execution.exchange.ShuffleExchangeLike);
  public static boolean shuffleParentReadsShuffleData$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.exchange.ShuffleExchangeLike, org.apache.spark.sql.execution.SparkPlan);
  public boolean shuffleParentReadsShuffleData(org.apache.spark.sql.execution.exchange.ShuffleExchangeLike, org.apache.spark.sql.execution.SparkPlan);
  public static org.apache.spark.sql.execution.SparkPlan addRowShuffleToQueryStageTransitionIfNeeded$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.ColumnarToRowTransition, org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec);
  public org.apache.spark.sql.execution.SparkPlan addRowShuffleToQueryStageTransitionIfNeeded(org.apache.spark.sql.execution.ColumnarToRowTransition, org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec);
  public static boolean checkCToRWithExecBroadcastAQECoalPart$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.SparkPlan, scala.Option);
  public boolean checkCToRWithExecBroadcastAQECoalPart(org.apache.spark.sql.execution.SparkPlan, scala.Option<org.apache.spark.sql.execution.SparkPlan>);
  public static scala.Option getShuffleFromCToRWithExecBroadcastAQECoalPart$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.SparkPlan);
  public scala.Option<org.apache.spark.sql.execution.SparkPlan> getShuffleFromCToRWithExecBroadcastAQECoalPart(org.apache.spark.sql.execution.SparkPlan);
  public static org.apache.spark.sql.execution.SparkPlan addExecBroadcastShuffle$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.SparkPlan);
  public org.apache.spark.sql.execution.SparkPlan addExecBroadcastShuffle(org.apache.spark.sql.execution.SparkPlan);
  public abstract scala.collection.immutable.Seq<org.apache.spark.sql.execution.SparkPlan> findOperators(org.apache.spark.sql.execution.SparkPlan, scala.Function1<org.apache.spark.sql.execution.SparkPlan, java.lang.Object>);
  public abstract boolean skipAssertIsOnTheGpu(org.apache.spark.sql.execution.SparkPlan);
  public abstract int leafNodeDefaultParallelism(org.apache.spark.sql.SparkSession);
  public abstract org.apache.spark.sql.execution.SparkPlan getAdaptiveInputPlan(org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec);
  public abstract com.nvidia.spark.rapids.ExecRule<? extends org.apache.spark.sql.execution.SparkPlan> neverReplaceShowCurrentNamespaceCommand();
  public abstract com.nvidia.spark.rapids.ExprRule<? extends org.apache.spark.sql.catalyst.expressions.Expression> ansiCastRule();
  public abstract boolean supportsColumnarAdaptivePlans();
  public abstract org.apache.spark.sql.execution.SparkPlan columnarAdaptivePlan(org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec, com.nvidia.spark.rapids.CoalesceSizeGoal);
  public static org.apache.spark.sql.execution.SparkPlan applyShimPlanRules$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.SparkPlan, com.nvidia.spark.rapids.RapidsConf);
  public org.apache.spark.sql.execution.SparkPlan applyShimPlanRules(org.apache.spark.sql.execution.SparkPlan, com.nvidia.spark.rapids.RapidsConf);
  public static org.apache.spark.sql.execution.SparkPlan applyPostShimPlanRules$(com.nvidia.spark.rapids.SparkShims, org.apache.spark.sql.execution.SparkPlan);
  public org.apache.spark.sql.execution.SparkPlan applyPostShimPlanRules(org.apache.spark.sql.execution.SparkPlan);
  public abstract boolean reproduceEmptyStringBug();
  public static void $init$(com.nvidia.spark.rapids.SparkShims);
}
