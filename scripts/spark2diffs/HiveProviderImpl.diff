20c20
< import com.nvidia.spark.rapids.{DataWritingCommandRule, ExecChecks, ExecRule, ExprChecks, ExprMeta, ExprRule, GpuExec, GpuExpression, GpuOverrides, HiveProvider, OptimizedCreateHiveTableAsSelectCommandMeta, RapidsConf, RepeatingParamCheck, SparkPlanMeta, TypeSig}
---
> import com.nvidia.spark.rapids.{DataWritingCommandRule, ExecChecks, ExecRule, ExprChecks, ExprMeta, ExprRule, GpuOverrides, HiveProvider, OptimizedCreateHiveTableAsSelectCommandMeta, RapidsConf, RepeatingParamCheck, SparkPlanMeta, TypeSig}
43c43,44
<    * Builds the expression rules that are specific to spark-hive Catalyst nodes.
---
>    * Builds the rules that are specific to spark-hive Catalyst nodes. This will return an empty
>    * mapping if spark-hive is unavailable.
67,86d67
< 
<           override def convertToGpu(): GpuExpression = {
<             opRapidsFunc.map { _ =>
<               // We use the original HiveGenericUDF `deterministic` method as a proxy
<               // for simplicity.
<               GpuHiveSimpleUDF(
<                 a.name,
<                 a.funcWrapper,
<                 childExprs.map(_.convertToGpu()),
<                 a.dataType,
<                 a.deterministic)
<             }.getOrElse {
<               // This `require` is just for double check.
<               require(conf.isCpuBasedUDFEnabled)
<               GpuRowBasedHiveSimpleUDF(
<                 a.name,
<                 a.funcWrapper,
<                 childExprs.map(_.convertToGpu()))
<             }
<           }
108,128d88
< 
<           override def convertToGpu(): GpuExpression = {
<             opRapidsFunc.map { _ =>
<               // We use the original HiveGenericUDF `deterministic` method as a proxy
<               // for simplicity.
<               GpuHiveGenericUDF(
<                 a.name,
<                 a.funcWrapper,
<                 childExprs.map(_.convertToGpu()),
<                 a.dataType,
<                 a.deterministic,
<                 a.foldable)
<             }.getOrElse {
<               // This `require` is just for double check.
<               require(conf.isCpuBasedUDFEnabled)
<               GpuRowBasedHiveGenericUDF(
<                 a.name,
<                 a.funcWrapper,
<                 childExprs.map(_.convertToGpu()))
<             }
<           }
192,202c152,157
<             if (tableRelation.isPartitioned) {
<               tableRelation.prunedPartitions.getOrElse(Seq.empty)
<                                             .map(_.storage)
<                                             .foreach(flagIfUnsupportedStorageFormat)
<             }
<           }
< 
<           override def convertToGpu(): GpuExec = {
<             GpuHiveTableScanExec(wrapped.requestedAttributes,
<               wrapped.relation,
<               wrapped.partitionPruningPred)
---
>             // no prunedPartitions in HiveTableRelation of spark 2.4
>             // if (tableRelation.isPartitioned) {
>               // tableRelation.prunedPartitions.getOrElse(Seq.empty)
>               //                              .map(_.storage)
>               //                              .foreach(flagIfUnsupportedStorageFormat)
>             // }
