21c21,30
< import org.apache.spark.sql.{SparkSession}
---
> import scala.util.control.NonFatal
> 
> import com.nvidia.spark.rapids.shims.CharVarcharUtilsShims
> 
> import org.apache.spark.sql.{SaveMode, SparkSession}
> import org.apache.spark.sql.catalyst.TableIdentifier
> import org.apache.spark.sql.catalyst.catalog.{CatalogTable, SessionCatalog}
> import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
> import org.apache.spark.sql.execution.SparkPlan
> import org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand
24c33,145
< import org.apache.spark.sql.rapids.{GpuOrcFileFormat}
---
> import org.apache.spark.sql.rapids.{GpuInsertIntoHadoopFsRelationCommand, GpuOrcFileFormat}
> import org.apache.spark.sql.rapids.execution.TrampolineUtil
> import org.apache.spark.sql.rapids.shims.RapidsErrorUtils
> import org.apache.spark.sql.vectorized.ColumnarBatch
> 
> /** GPU version of Spark's CreateHiveTableAsSelectBase */
> trait GpuCreateHiveTableAsSelectBase extends GpuDataWritingCommand {
>   val tableDesc: CatalogTable
>   val query: LogicalPlan
>   val outputColumnNames: Seq[String]
>   val mode: SaveMode
> 
>   protected val tableIdentifier: TableIdentifier = tableDesc.identifier
> 
>   override def runColumnar(sparkSession: SparkSession, child: SparkPlan): Seq[ColumnarBatch] = {
>     val catalog = sparkSession.sessionState.catalog
>     val tableExists = catalog.tableExists(tableIdentifier)
> 
>     if (tableExists) {
>       assert(mode != SaveMode.Overwrite,
>         s"Expect the table $tableIdentifier has been dropped when the save mode is Overwrite")
> 
>       if (mode == SaveMode.ErrorIfExists) {
>         throw RapidsErrorUtils.tableIdentifierExistsError(tableIdentifier)
>       }
>       if (mode == SaveMode.Ignore) {
>         // Since the table already exists and the save mode is Ignore, we will just return.
>         return Seq.empty
>       }
> 
>       val command = getWritingCommand(catalog, tableDesc, tableExists = true)
>       command.runColumnar(sparkSession, child)
>       GpuDataWritingCommand.propogateMetrics(sparkSession.sparkContext, command, metrics)
>     } else {
>       tableDesc.storage.locationUri.foreach { p =>
>         GpuDataWritingCommand.assertEmptyRootPath(p, mode, sparkSession.sessionState.newHadoopConf)
>       }
>       // TODO ideally, we should get the output data ready first and then
>       // add the relation into catalog, just in case of failure occurs while data
>       // processing.
>       val tableSchema = CharVarcharUtilsShims.getRawSchema(
>         outputColumns.toStructType, sparkSession.sessionState.conf)
>       assert(tableDesc.schema.isEmpty)
>       catalog.createTable(
>         tableDesc.copy(schema = tableSchema), ignoreIfExists = false)
> 
>       try {
>         // Read back the metadata of the table which was created just now.
>         val createdTableMeta = catalog.getTableMetadata(tableDesc.identifier)
>         val command = getWritingCommand(catalog, createdTableMeta, tableExists = false)
>         command.runColumnar(sparkSession, child)
>         GpuDataWritingCommand.propogateMetrics(sparkSession.sparkContext, command, metrics)
>       } catch {
>         case NonFatal(e) =>
>           // drop the created table.
>           catalog.dropTable(tableIdentifier, ignoreIfNotExists = true, purge = false)
>           throw e
>       }
>     }
> 
>     Seq.empty[ColumnarBatch]
>   }
> 
>   // Returns `GpuDataWritingCommand` which actually writes data into the table.
>   def getWritingCommand(
>       catalog: SessionCatalog,
>       tableDesc: CatalogTable,
>       tableExists: Boolean): GpuDataWritingCommand
> 
>   // A subclass should override this with the Class name of the concrete type expected to be
>   // returned from `getWritingCommand`.
>   def writingCommandClassName: String
> 
>   override def argString(maxFields: Int): String = {
>     s"[Database: ${tableDesc.database}, " +
>         s"TableName: ${tableDesc.identifier.table}, " +
>         s"$writingCommandClassName]"
>   }
> }
> 
> case class GpuOptimizedCreateHiveTableAsSelectCommand(
>     tableDesc: CatalogTable,
>     query: LogicalPlan,
>     outputColumnNames: Seq[String],
>     mode: SaveMode,
>     cpuCmd: OptimizedCreateHiveTableAsSelectCommand) extends GpuCreateHiveTableAsSelectBase {
>   override def getWritingCommand(
>       catalog: SessionCatalog,
>       tableDesc: CatalogTable,
>       tableExists: Boolean): GpuDataWritingCommand = {
>     // Leverage the existing support for InsertIntoHadoopFsRelationCommand to do the write
>     cpuCmd.getWritingCommand(catalog, tableDesc, tableExists) match {
>       case c: InsertIntoHadoopFsRelationCommand =>
>         val rapidsConf = new RapidsConf(conf)
>         val rule = GpuOverrides.dataWriteCmds(c.getClass)
>         val meta = new InsertIntoHadoopFsRelationCommandMeta(c, rapidsConf, None, rule)
>         meta.tagForGpu()
>         if (!meta.canThisBeReplaced) {
>           throw new IllegalStateException("Unable to convert writing command: " +
>               meta.explain(all = false))
>         }
>         meta.convertToGpu()
>       case c =>
>         throw new UnsupportedOperationException(s"Unsupported write command: $c")
>     }
>   }
> 
>   override def writingCommandClassName: String =
>     TrampolineUtil.getSimpleName(classOf[GpuInsertIntoHadoopFsRelationCommand])
> 
>   // Do not support partitioned or bucketed writes
>   override def requireSingleBatch: Boolean = false
> }
29c150
<     parent: Option[RapidsMeta[_, _]],
---
>     parent: Option[RapidsMeta[_, _, _]],
68a190,198
>   }
> 
>   override def convertToGpu(): GpuDataWritingCommand = {
>     GpuOptimizedCreateHiveTableAsSelectCommand(
>       wrapped.tableDesc,
>       wrapped.query,
>       wrapped.outputColumnNames,
>       wrapped.mode,
>       cmd)
