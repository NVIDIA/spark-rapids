17a18,20
> import ai.rapids.cudf
> import ai.rapids.cudf.DType
> import com.nvidia.spark.rapids.GpuRowToColumnConverter.TypeConverter
31a35,52
>   /**
>    * Get the TypeConverter of the data type for this Shim
>    * Note should first calling hasConverterForType
>    * @param t the data type
>    * @param nullable is nullable
>    * @return the row to column convert for the data type
>    */
>   def getConverterForType(t: DataType, nullable: Boolean): TypeConverter = {
>     throw new RuntimeException(s"No converter is found for type $t.")
>   }
> 
>   /**
>    * Get the cuDF type for the Spark data type
>    * @param t the Spark data type
>    * @return the cuDF type if the Shim supports
>    */
>   def toRapidsOrNull(t: DataType): DType = null
> 
34a56,71
>   /**
>    * Copy a column for computing on GPU.
>    * Better to check if the type is supported first by calling 'isColumnarCopySupportedForType'
>    *
>    * Data type is passed explicitly to allow overriding the reported type from the column vector.
>    * There are cases where the type reported by the column vector does not match the data.
>    * See https://github.com/apache/iceberg/issues/6116.
>    */
>   def columnarCopy(
>       cv: ColumnVector,
>       b: ai.rapids.cudf.HostColumnVector.ColumnBuilder,
>       dataType: DataType,
>       rows: Int): Unit = {
>     throw new UnsupportedOperationException(s"Converting to GPU for $dataType is not supported yet")
>   }
> 
49a87,89
> 
>   def csvRead(cv: cudf.ColumnVector, dt: DataType): cudf.ColumnVector =
>     throw new RuntimeException(s"Not support type $dt.")
