2c2
<  * Copyright (c) 2022, NVIDIA CORPORATION.
---
>  * Copyright (c) 2019-2022, NVIDIA CORPORATION.
24a25
> import ai.rapids.cudf.DType
26c27
< import com.nvidia.spark.rapids.shims.{GpuBroadcastHashJoinMeta, GpuShuffledHashJoinMeta, GpuSortMergeJoinMeta, GpuSpecifiedWindowFrameMeta, GpuTypeShims, GpuWindowExpressionMeta, GpuWindowSpecDefinitionMeta, OffsetWindowFunctionMeta}
---
> import com.nvidia.spark.rapids.shims.{AQEUtils, GpuBatchScanExec, GpuHashPartitioning, GpuRangePartitioning, GpuSpecifiedWindowFrameMeta, GpuTypeShims, GpuWindowExpressionMeta, OffsetWindowFunctionMeta, SparkShimImpl}
28a30
> import org.apache.spark.rapids.shims.GpuShuffleExchangeExec
32a35,36
> import org.apache.spark.sql.catalyst.json.rapids.GpuJsonScan
> import org.apache.spark.sql.catalyst.optimizer.NormalizeNaNAndZero
34a39
> import org.apache.spark.sql.catalyst.trees.TreeNodeTag
35a41
> import org.apache.spark.sql.connector.read.Scan
37,38c43,44
< import org.apache.spark.sql.execution.ScalarSubquery
< import org.apache.spark.sql.execution.aggregate._
---
> import org.apache.spark.sql.execution.adaptive.{AdaptiveSparkPlanExec, BroadcastQueryStageExec, ShuffleQueryStageExec}
> import org.apache.spark.sql.execution.aggregate.{HashAggregateExec, ObjectHashAggregateExec, SortAggregateExec}
46c52,55
< import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, ReusedExchangeExec, ShuffleExchangeExec}
---
> import org.apache.spark.sql.execution.datasources.v2._
> import org.apache.spark.sql.execution.datasources.v2.csv.CSVScan
> import org.apache.spark.sql.execution.datasources.v2.json.JsonScan
> import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, ENSURE_REQUIREMENTS, ReusedExchangeExec, ShuffleExchangeExec}
52a62
> import org.apache.spark.sql.rapids.catalyst.expressions.GpuRand
53a64,66
> import org.apache.spark.sql.rapids.execution.python._
> import org.apache.spark.sql.rapids.execution.python.shims.GpuFlatMapGroupsInPandasExecMeta
> import org.apache.spark.sql.rapids.shims.GpuTimeAdd
66c79
< abstract class ReplacementRule[INPUT <: BASE, BASE, WRAP_TYPE <: RapidsMeta[INPUT, BASE]](
---
> abstract class ReplacementRule[INPUT <: BASE, BASE, WRAP_TYPE <: RapidsMeta[INPUT, BASE, _]](
70c83
<         Option[RapidsMeta[_, _]],
---
>         Option[RapidsMeta[_, _, _]],
121c134
<       Option[RapidsMeta[_, _]],
---
>       Option[RapidsMeta[_, _, _]],
193c206
<       parent: Option[RapidsMeta[_, _]],
---
>       parent: Option[RapidsMeta[_, _, _]],
208c221
<         Option[RapidsMeta[_, _]],
---
>         Option[RapidsMeta[_, _, _]],
223d235
< /*
238c250
< */
---
> 
246c258
<         Option[RapidsMeta[_, _]],
---
>         Option[RapidsMeta[_, _, _]],
265c277
<         Option[RapidsMeta[_, _]],
---
>         Option[RapidsMeta[_, _, _]],
287c299
<         Option[RapidsMeta[_, _]],
---
>         Option[RapidsMeta[_, _, _]],
298d309
< 
302c313
<     parent: Option[RapidsMeta[_, _]],
---
>     parent: Option[RapidsMeta[_, _, _]],
306,310c317
<   // spark 2.3 doesn't have this so just code it here
<   def sparkSessionActive: SparkSession = {
<     SparkSession.getActiveSession.getOrElse(SparkSession.getDefaultSession.getOrElse(
<       throw new IllegalStateException("No active or default Spark session found")))
<   }
---
>   private var fileFormat: Option[ColumnarFileFormat] = None
317c324
<     val spark = sparkSessionActive
---
>     val spark = SparkSession.active
319c326
<     cmd.fileFormat match {
---
>     fileFormat = cmd.fileFormat match {
321a329
>         None
323a332
>         None
329a339
>         None
331a342
>         None
334d344
< }
335a346,366
>   override def convertToGpu(): GpuDataWritingCommand = {
>     val format = fileFormat.getOrElse(
>       throw new IllegalStateException("fileFormat missing, tagSelfForGpu not called?"))
> 
>     GpuInsertIntoHadoopFsRelationCommand(
>       cmd.outputPath,
>       cmd.staticPartitions,
>       cmd.ifPartitionNotExists,
>       cmd.partitionColumns,
>       cmd.bucketSpec,
>       format,
>       cmd.options,
>       cmd.query,
>       cmd.mode,
>       cmd.catalogTable,
>       cmd.fileIndex,
>       cmd.outputColumnNames,
>       conf.stableSort,
>       conf.concurrentWriterPartitionFlushSize)
>   }
> }
340c371
<     parent: Option[RapidsMeta[_, _]],
---
>     parent: Option[RapidsMeta[_, _, _]],
345,350c376
< 
<   // spark 2.3 doesn't have this so just code it here
<   def sparkSessionActive: SparkSession = {
<     SparkSession.getActiveSession.getOrElse(SparkSession.getDefaultSession.getOrElse(
<       throw new IllegalStateException("No active or default Spark session found")))
<   }
---
>   private var gpuProvider: Option[ColumnarFileFormat] = None
360c386
<     val spark = sparkSessionActive
---
>     val spark = SparkSession.active
362c388
<       GpuDataSource.lookupDataSource(cmd.table.provider.get, spark.sessionState.conf)
---
>       GpuDataSource.lookupDataSourceWithFallback(cmd.table.provider.get, spark.sessionState.conf)
365c391
<     origProvider.getConstructor().newInstance() match {
---
>     gpuProvider = origProvider.getConstructor().newInstance() match {
368d393
<         None
372d396
<         None
378a403,416
>   override def convertToGpu(): GpuDataWritingCommand = {
>     val newProvider = gpuProvider.getOrElse(
>       throw new IllegalStateException("fileFormat unexpected, tagSelfForGpu not called?"))
> 
>     GpuCreateDataSourceTableAsSelectCommand(
>       cmd.table,
>       cmd.mode,
>       cmd.query,
>       cmd.outputColumnNames,
>       origProvider,
>       newProvider,
>       conf.stableSort,
>       conf.concurrentWriterPartitionFlushSize)
>   }
380a419,421
> /**
>  * Listener trait so that tests can confirm that the expected optimizations are being applied
>  */
398d438
< 
402d441
< 
406d444
< 
419,421d456
< // copy here for 2.x
< sealed abstract class Optimization
< 
423,427d457
<   // Spark 2.x - don't pull in cudf so hardcode here
<   val DECIMAL32_MAX_PRECISION = 9
<   val DECIMAL64_MAX_PRECISION = 18
<   val DECIMAL128_MAX_PRECISION = 38
< 
480a511,573
>   private def convertPartToGpuIfPossible(part: Partitioning, conf: RapidsConf): Partitioning = {
>     part match {
>       case _: GpuPartitioning => part
>       case _ =>
>         val wrapped = wrapPart(part, conf, None)
>         wrapped.tagForGpu()
>         if (wrapped.canThisBeReplaced) {
>           wrapped.convertToGpu()
>         } else {
>           part
>         }
>     }
>   }
> 
>   /**
>    * Removes unnecessary CPU shuffles that Spark can add to the plan when it does not realize
>    * a GPU partitioning satisfies a CPU distribution because CPU and GPU expressions are not
>    * semantically equal.
>    */
>   def removeExtraneousShuffles(plan: SparkPlan, conf: RapidsConf): SparkPlan = {
>     plan.transformUp {
>       case cpuShuffle: ShuffleExchangeExec =>
>         cpuShuffle.child match {
>           case sqse: ShuffleQueryStageExec =>
>             GpuTransitionOverrides.getNonQueryStagePlan(sqse) match {
>               case gpuShuffle: GpuShuffleExchangeExecBase =>
>                 val converted = convertPartToGpuIfPossible(cpuShuffle.outputPartitioning, conf)
>                 if (converted == gpuShuffle.outputPartitioning) {
>                   sqse
>                 } else {
>                   cpuShuffle
>                 }
>               case _ => cpuShuffle
>             }
>           case _ => cpuShuffle
>         }
>     }
>   }
> 
>   /**
>    * Searches the plan for ReusedExchangeExec instances containing a GPU shuffle where the
>    * output types between the two plan nodes do not match. In such a case the ReusedExchangeExec
>    * will be updated to match the GPU shuffle output types.
>    */
>   def fixupReusedExchangeExecs(plan: SparkPlan): SparkPlan = {
>     def outputTypesMatch(a: Seq[Attribute], b: Seq[Attribute]): Boolean =
>       a.corresponds(b)((x, y) => x.dataType == y.dataType)
>     plan.transformUp {
>       case sqse: ShuffleQueryStageExec =>
>         sqse.plan match {
>           case ReusedExchangeExec(output, gsee: GpuShuffleExchangeExecBase) if (
>               !outputTypesMatch(output, gsee.output)) =>
>             val newOutput = sqse.plan.output.zip(gsee.output).map { case (c, g) =>
>               assert(c.isInstanceOf[AttributeReference] && g.isInstanceOf[AttributeReference],
>                 s"Expected AttributeReference but found $c and $g")
>               AttributeReference(c.name, g.dataType, c.nullable, c.metadata)(c.exprId, c.qualifier)
>             }
>             AQEUtils.newReuseInstance(sqse, newOutput)
>           case _ => sqse
>         }
>     }
>   }
> 
573c666
<       case dt: DecimalType if allowDecimal => dt.precision <= GpuOverrides.DECIMAL64_MAX_PRECISION
---
>       case dt: DecimalType if allowDecimal => dt.precision <= DType.DECIMAL64_MAX_PRECISION
596c689
<   def checkAndTagFloatAgg(dataType: DataType, conf: RapidsConf, meta: RapidsMeta[_,_]): Unit = {
---
>   def checkAndTagFloatAgg(dataType: DataType, conf: RapidsConf, meta: RapidsMeta[_,_,_]): Unit = {
605a699,730
>   /**
>    * Helper function specific to ANSI mode for the aggregate functions that should
>    * fallback, since we don't have the same overflow checks that Spark provides in
>    * the CPU
>    * @param checkType Something other than `None` triggers logic to detect whether
>    *                  the agg should fallback in ANSI mode. Otherwise (None), it's
>    *                  an automatic fallback.
>    * @param meta agg expression meta
>    */
>   def checkAndTagAnsiAgg(checkType: Option[DataType], meta: AggExprMeta[_]): Unit = {
>     val failOnError = SQLConf.get.ansiEnabled
>     if (failOnError) {
>       if (checkType.isDefined) {
>         val typeToCheck = checkType.get
>         val failedType = typeToCheck match {
>           case _: DecimalType | LongType | IntegerType | ShortType | ByteType => true
>           case _ =>  false
>         }
>         if (failedType) {
>           meta.willNotWorkOnGpu(
>             s"ANSI mode not supported for ${meta.expr} with $typeToCheck result type")
>         }
>       } else {
>         // Average falls into this category, where it produces Doubles, but
>         // internally it uses Double and Long, and Long could overflow (technically)
>         // and failOnError given that it is based on catalyst Add.
>         meta.willNotWorkOnGpu(
>           s"ANSI mode not supported for ${meta.expr}")
>       }
>     }
>   }
> 
609c734
<       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _]], DataFromReplacementRule)
---
>       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _, _]], DataFromReplacementRule)
616a742,751
>   def scan[INPUT <: Scan](
>       desc: String,
>       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _, _]], DataFromReplacementRule)
>           => ScanMeta[INPUT])
>       (implicit tag: ClassTag[INPUT]): ScanRule[INPUT] = {
>     assert(desc != null)
>     assert(doWrap != null)
>     new ScanRule[INPUT](doWrap, desc, tag)
>   }
> 
620c755
<       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _]], DataFromReplacementRule)
---
>       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _, _]], DataFromReplacementRule)
638c773
<         p: Option[RapidsMeta[_, _]],
---
>         p: Option[RapidsMeta[_, _, _]],
647c782
<       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _]], DataFromReplacementRule)
---
>       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _, _]], DataFromReplacementRule)
657c792
<       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _]], DataFromReplacementRule)
---
>       doWrap: (INPUT, RapidsConf, Option[RapidsMeta[_, _, _]], DataFromReplacementRule)
668c803
<       parent: Option[RapidsMeta[_, _]]): BaseExprMeta[INPUT] =
---
>       parent: Option[RapidsMeta[_, _, _]]): BaseExprMeta[INPUT] =
728a864
>         override def convertToGpu(child: Expression): GpuExpression = GpuSignum(child)
738a875,876
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuAlias(child, a.name)(a.exprId, a.qualifier, a.explicitMetadata)
749a888
>         override def convertToGpu(): Expression = att
763a903
>         override def convertToGpu(child: Expression): GpuExpression = GpuPromotePrecision(child)
781,782c921
<                   // allowNegativeScaleOfDecimalEnabled is not in 2.x assume its default false
<                   val t = if (s < 0 && !false) {
---
>                   val t = if (s < 0 && !SQLConf.get.allowNegativeScaleOfDecimalEnabled) {
794,795c933
<             // Spark 2.X only has Cast, no AnsiCast so no CastBase, hardcode here to Cast
<             case p: PromotePrecision if p.child.isInstanceOf[Cast] &&
---
>             case p: PromotePrecision if p.child.isInstanceOf[CastBase] &&
797c935
<               val c = p.child.asInstanceOf[Cast]
---
>               val c = p.child.asInstanceOf[CastBase]
836a975,1002
>         override def convertToGpu(): GpuExpression = {
>           // Prior to Spark 3.4.0
>           // Division and Multiplication of Decimal types is a little odd. Spark will cast the
>           // inputs to a common wider value where the scale is the max of the two input scales,
>           // and the precision is max of the two input non-scale portions + the new scale. Then it
>           // will do the divide or multiply as a BigDecimal value but lie about the return type.
>           // Finally here in CheckOverflow it will reset the scale and check the precision so that
>           // Spark knows it fits in the final desired result.
>           // Here we try to strip out the extra casts, etc to get to as close to the original
>           // query as possible. This lets us then calculate what CUDF needs to get the correct
>           // answer, which in some cases is a lot smaller.
> 
>           a.child match {
>             case _: Divide =>
>               // GpuDecimalDivide includes the overflow check in it.
>               GpuDecimalDivide(lhs.convertToGpu(), rhs.convertToGpu(), wrapped.dataType)
>             case _: Multiply =>
>               // GpuDecimal*Multiply includes the overflow check in it
>               val intermediatePrecision =
>                 GpuDecimalMultiply.nonRoundedIntermediatePrecision(lhsDecimalType,
>                   rhsDecimalType, a.dataType)
>               GpuDecimalMultiply(lhs.convertToGpu(), rhs.convertToGpu(), wrapped.dataType,
>                 useLongMultiply = intermediatePrecision > DType.DECIMAL128_MAX_PRECISION)
>             case _ =>
>               GpuCheckOverflow(childExprs.head.convertToGpu(),
>                 wrapped.dataType, wrapped.nullOnOverflow)
>           }
>         }
841a1008
>         override def convertToGpu(child: Expression): GpuToDegrees = GpuToDegrees(child)
846a1014
>         override def convertToGpu(child: Expression): GpuToRadians = GpuToRadians(child)
885a1054
>         override def convertToGpu(): GpuExpression = GpuSpecialFrameBoundary(currentRow)
891a1061
>           override def convertToGpu(): GpuExpression = GpuSpecialFrameBoundary(unboundedPreceding)
897a1068
>           override def convertToGpu(): GpuExpression = GpuSpecialFrameBoundary(unboundedFollowing)
902a1074
>         override def convertToGpu(): GpuExpression = GpuRowNumber
911a1084
>         override def convertToGpu(): GpuExpression = GpuRank(childExprs.map(_.convertToGpu()))
920a1094
>         override def convertToGpu(): GpuExpression = GpuDenseRank(childExprs.map(_.convertToGpu()))
929a1104,1105
>         override def convertToGpu(): GpuExpression =
>           GpuPercentRank(childExprs.map(_.convertToGpu()))
949a1126,1127
>         override def convertToGpu(): GpuExpression =
>           GpuLead(input.convertToGpu(), offset.convertToGpu(), default.convertToGpu())
969a1148,1149
>         override def convertToGpu(): GpuExpression =
>           GpuLag(input.convertToGpu(), offset.convertToGpu(), default.convertToGpu())
979a1160,1161
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuPreciseTimestampConversion(child, a.fromType, a.toType)
988,989c1170
<         // val ansiEnabled = SQLConf.get.ansiEnabled
<         val ansiEnabled = false
---
>         val ansiEnabled = SQLConf.get.ansiEnabled
992,993d1172
<           // Spark 2.x - ansi in not in 2.x
<           /*
997,998d1175
< 
<            */
999a1177,1179
> 
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuUnaryMinus(child, ansiEnabled)
1007a1188
>         override def convertToGpu(child: Expression): GpuExpression = GpuUnaryPositive(child)
1012a1194
>         override def convertToGpu(child: Expression): GpuExpression = GpuYear(child)
1017a1200
>         override def convertToGpu(child: Expression): GpuExpression = GpuMonth(child)
1022a1206
>         override def convertToGpu(child: Expression): GpuExpression = GpuQuarter(child)
1027a1212
>         override def convertToGpu(child: Expression): GpuExpression = GpuDayOfMonth(child)
1032a1218
>         override def convertToGpu(child: Expression): GpuExpression = GpuDayOfYear(child)
1037a1224,1235
>         override def convertToGpu(child: Expression): GpuExpression = GpuAcos(child)
>       }),
>     expr[Acosh](
>       "Inverse hyperbolic cosine",
>       ExprChecks.mathUnaryWithAst,
>       (a, conf, p, r) => new UnaryAstExprMeta[Acosh](a, conf, p, r) {
>         override def convertToGpu(child: Expression): GpuExpression =
>           if (conf.includeImprovedFloat) {
>             GpuAcoshImproved(child)
>           } else {
>             GpuAcoshCompat(child)
>           }
1039d1236
<     // Acosh is not supported in spark 2.x
1043a1241,1261
>         override def convertToGpu(child: Expression): GpuExpression = GpuAsin(child)
>       }),
>     expr[Asinh](
>       "Inverse hyperbolic sine",
>       ExprChecks.mathUnaryWithAst,
>       (a, conf, p, r) => new UnaryAstExprMeta[Asinh](a, conf, p, r) {
>         override def convertToGpu(child: Expression): GpuExpression =
>           if (conf.includeImprovedFloat) {
>             GpuAsinhImproved(child)
>           } else {
>             GpuAsinhCompat(child)
>           }
> 
>         override def tagSelfForAst(): Unit = {
>           if (!conf.includeImprovedFloat) {
>             // AST is not expressive enough yet to implement the conditional expression needed
>             // to emulate Spark's behavior
>             willNotWorkInAst("asinh is not AST compatible unless " +
>                 s"${RapidsConf.IMPROVED_FLOAT_OPS.key} is enabled")
>           }
>         }
1045d1262
<     // Asinh is not supported in spark 2.x
1049a1267
>         override def convertToGpu(child: Expression): GpuExpression = GpuSqrt(child)
1054a1273
>         override def convertToGpu(child: Expression): GpuExpression = GpuCbrt(child)
1063a1283,1284
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuHypot(lhs, rhs)
1075c1296
<               if (precision > GpuOverrides.DECIMAL128_MAX_PRECISION) {
---
>               if (precision > DType.DECIMAL128_MAX_PRECISION) {
1082a1304,1307
>         override def convertToGpu(child: Expression): GpuExpression = {
>           // use Spark `Floor.dataType` to keep consistent between Spark versions.
>           GpuFloor(child, a.dataType)
>         }
1094c1319
<               if (precision > GpuOverrides.DECIMAL128_MAX_PRECISION) {
---
>               if (precision > DType.DECIMAL128_MAX_PRECISION) {
1101a1327,1330
>         override def convertToGpu(child: Expression): GpuExpression = {
>           // use Spark `Ceil.dataType` to keep consistent between Spark versions.
>           GpuCeil(child, a.dataType)
>         }
1107a1337
>         override def convertToGpu(child: Expression): GpuExpression = GpuNot(child)
1116a1347
>         override def convertToGpu(child: Expression): GpuExpression = GpuIsNull(child)
1125a1357
>         override def convertToGpu(child: Expression): GpuExpression = GpuIsNotNull(child)
1131a1364
>         override def convertToGpu(child: Expression): GpuExpression = GpuIsNan(child)
1136a1370
>         override def convertToGpu(child: Expression): GpuExpression = GpuRint(child)
1142a1377
>         override def convertToGpu(child: Expression): GpuExpression = GpuBitwiseNot(child)
1151a1387,1388
>         def convertToGpu(): GpuExpression =
>           GpuAtLeastNNonNulls(a.n, childExprs.map(_.convertToGpu()))
1160a1398,1399
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuDateAdd(lhs, rhs)
1169a1409,1410
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuDateSub(lhs, rhs)
1176a1418,1419
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuNaNvl(lhs, rhs)
1183a1427,1428
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuShiftLeft(lhs, rhs)
1190a1436,1437
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuShiftRight(lhs, rhs)
1197a1445,1446
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuShiftRightUnsigned(lhs, rhs)
1205a1455,1456
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuBitwiseAnd(lhs, rhs)
1213a1465,1466
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuBitwiseOr(lhs, rhs)
1221a1475,1476
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuBitwiseXor(lhs, rhs)
1233a1489
>         override def convertToGpu(): GpuExpression = GpuCoalesce(childExprs.map(_.convertToGpu()))
1242a1499
>         override def convertToGpu(): GpuExpression = GpuLeast(childExprs.map(_.convertToGpu()))
1251a1509
>         override def convertToGpu(): GpuExpression = GpuGreatest(childExprs.map(_.convertToGpu()))
1256a1515,1521
>         override def convertToGpu(child: Expression): GpuExpression = GpuAtan(child)
>       }),
>     expr[Atanh](
>       "Inverse hyperbolic tangent",
>       ExprChecks.mathUnaryWithAst,
>       (a, conf, p, r) => new UnaryAstExprMeta[Atanh](a, conf, p, r) {
>         override def convertToGpu(child: Expression): GpuExpression = GpuAtanh(child)
1258d1522
<     // Atanh is not supported in spark 2.x
1262a1527
>         override def convertToGpu(child: Expression): GpuExpression = GpuCos(child)
1267a1533
>         override def convertToGpu(child: Expression): GpuExpression = GpuExp(child)
1272a1539
>         override def convertToGpu(child: Expression): GpuExpression = GpuExpm1(child)
1278a1546
>         override def convertToGpu(child: Expression): GpuExpression = GpuInitCap(child)
1283a1552
>         override def convertToGpu(child: Expression): GpuExpression = GpuLog(child)
1288a1558,1562
>         override def convertToGpu(child: Expression): GpuExpression = {
>           // No need for overflow checking on the GpuAdd in Double as Double handles overflow
>           // the same in all modes.
>           GpuLog(GpuAdd(child, GpuLiteral(1d, DataTypes.DoubleType), false))
>         }
1293a1568,1569
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuLogarithm(child, GpuLiteral(2d, DataTypes.DoubleType))
1298a1575,1576
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuLogarithm(child, GpuLiteral(10d, DataTypes.DoubleType))
1305a1584,1586
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           // the order of the parameters is transposed intentionally
>           GpuLogarithm(rhs, lhs)
1310a1592
>         override def convertToGpu(child: Expression): GpuExpression = GpuSin(child)
1315a1598
>         override def convertToGpu(child: Expression): GpuExpression = GpuSinh(child)
1320a1604
>         override def convertToGpu(child: Expression): GpuExpression = GpuCosh(child)
1325a1610
>         override def convertToGpu(child: Expression): GpuExpression = GpuCot(child)
1330a1616
>         override def convertToGpu(child: Expression): GpuExpression = GpuTanh(child)
1335a1622,1640
>         override def convertToGpu(child: Expression): GpuExpression = GpuTan(child)
>       }),
>     expr[NormalizeNaNAndZero](
>       "Normalize NaN and zero",
>       ExprChecks.unaryProjectInputMatchesOutput(
>         TypeSig.DOUBLE + TypeSig.FLOAT,
>         TypeSig.DOUBLE + TypeSig.FLOAT),
>       (a, conf, p, r) => new UnaryExprMeta[NormalizeNaNAndZero](a, conf, p, r) {
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuNormalizeNaNAndZero(child)
>       }),
>     expr[KnownFloatingPointNormalized](
>       "Tag to prevent redundant normalization",
>       ExprChecks.unaryProjectInputMatchesOutput(
>         TypeSig.DOUBLE + TypeSig.FLOAT,
>         TypeSig.DOUBLE + TypeSig.FLOAT),
>       (a, conf, p, r) => new UnaryExprMeta[KnownFloatingPointNormalized](a, conf, p, r) {
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuKnownFloatingPointNormalized(child)
1337,1338d1641
<     // NormalizeNaNAndZero is not supported in spark 2.x
<     // KnownFloatingPointNormalized is not supported in spark 2.x
1344a1648,1649
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuKnownNotNull(child)
1351a1657,1659
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           GpuDateDiff(lhs, rhs)
>         }
1369a1678,1679
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuTimeAdd(lhs, rhs)
1371c1681,1701
<     // DateAddInterval is not supported in spark 2.x
---
>     expr[DateAddInterval](
>       "Adds interval to date",
>       ExprChecks.binaryProject(TypeSig.DATE, TypeSig.DATE,
>         ("start", TypeSig.DATE, TypeSig.DATE),
>         ("interval", TypeSig.lit(TypeEnum.CALENDAR)
>           .withPsNote(TypeEnum.CALENDAR, "month intervals are not supported"),
>           TypeSig.CALENDAR)),
>       (dateAddInterval, conf, p, r) =>
>         new BinaryExprMeta[DateAddInterval](dateAddInterval, conf, p, r) {
>           override def tagExprForGpu(): Unit = {
>             GpuOverrides.extractLit(dateAddInterval.interval).foreach { lit =>
>               val intvl = lit.value.asInstanceOf[CalendarInterval]
>               if (intvl.months != 0) {
>                 willNotWorkOnGpu("interval months isn't supported")
>               }
>             }
>           }
> 
>           override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>             GpuDateAddInterval(lhs, rhs)
>         }),
1379a1710,1711
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuDateFormatClass(lhs, rhs, strfFormat)
1391a1724,1731
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           if (conf.isImprovedTimestampOpsEnabled) {
>             // passing the already converted strf string for a little optimization
>             GpuToUnixTimestampImproved(lhs, rhs, sparkFormat, strfFormat)
>           } else {
>             GpuToUnixTimestamp(lhs, rhs, sparkFormat, strfFormat)
>           }
>         }
1402a1743,1750
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           if (conf.isImprovedTimestampOpsEnabled) {
>             // passing the already converted strf string for a little optimization
>             GpuUnixTimestampImproved(lhs, rhs, sparkFormat, strfFormat)
>           } else {
>             GpuUnixTimestamp(lhs, rhs, sparkFormat, strfFormat)
>           }
>         }
1409a1758
>         override def convertToGpu(expr: Expression): GpuExpression = GpuHour(expr)
1415a1765,1767
> 
>         override def convertToGpu(expr: Expression): GpuExpression =
>           GpuMinute(expr)
1421a1774,1776
> 
>         override def convertToGpu(expr: Expression): GpuExpression =
>           GpuSecond(expr)
1427a1783,1784
>         override def convertToGpu(expr: Expression): GpuExpression =
>           GpuWeekDay(expr)
1433a1791,1792
>         override def convertToGpu(expr: Expression): GpuExpression =
>           GpuDayOfWeek(expr)
1438a1798,1799
>         override def convertToGpu(expr: Expression): GpuExpression =
>           GpuLastDay(expr)
1447a1809,1811
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           // passing the already converted strf string for a little optimization
>           GpuFromUnixTime(lhs, rhs, strfFormat)
1472a1837,1838
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuPmod(lhs, rhs)
1485c1851
<         private val ansiEnabled = false
---
>         private val ansiEnabled = SQLConf.get.ansiEnabled
1487a1854,1856
>           if (ansiEnabled && GpuAnsi.needBasicOpOverflowCheck(a.dataType)) {
>             willNotWorkInAst("AST Addition does not support ANSI mode.")
>           }
1489a1859,1860
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuAdd(lhs, rhs, failOnError = ansiEnabled)
1502c1873
<         private val ansiEnabled = false
---
>         private val ansiEnabled = SQLConf.get.ansiEnabled
1504a1876,1878
>           if (ansiEnabled && GpuAnsi.needBasicOpOverflowCheck(a.dataType)) {
>             willNotWorkInAst("AST Subtraction does not support ANSI mode.")
>           }
1506a1881,1882
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuSubtract(lhs, rhs, ansiEnabled)
1519a1896,1898
>           if (SQLConf.get.ansiEnabled && GpuAnsi.needBasicOpOverflowCheck(a.dataType)) {
>             willNotWorkOnGpu("GPU Multiplication does not support ANSI mode")
>           }
1521a1901,1908
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           a.dataType match {
>             case _: DecimalType => throw new IllegalStateException(
>               "Decimal Multiply should be converted in CheckOverflow")
>             case _ =>
>               GpuMultiply(lhs, rhs)
>           }
>         }
1528a1916,1917
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuAnd(lhs, rhs)
1535a1925,1926
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuOr(lhs, rhs)
1547a1939,1940
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuEqualNullSafe(lhs, rhs)
1560a1954,1955
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuEqualTo(lhs, rhs)
1573a1969,1970
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuGreaterThan(lhs, rhs)
1586a1984,1985
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuGreaterThanOrEqual(lhs, rhs)
1606a2006,2007
>         override def convertToGpu(): GpuExpression =
>           GpuInSet(childExprs.head.convertToGpu(), in.list.asInstanceOf[Seq[Literal]].map(_.value))
1617a2019,2020
>         override def convertToGpu(): GpuExpression =
>           GpuInSet(childExprs.head.convertToGpu(), in.hset.toSeq)
1630a2034,2035
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuLessThan(lhs, rhs)
1643a2049,2050
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuLessThanOrEqual(lhs, rhs)
1648a2056,2067
>         override def convertToGpu(): GpuExpression = {
>           val branches = childExprs.grouped(2).flatMap {
>             case Seq(cond, value) => Some((cond.convertToGpu(), value.convertToGpu()))
>             case Seq(_) => None
>           }.toArray.toSeq  // force materialization to make the seq serializable
>           val elseValue = if (childExprs.size % 2 != 0) {
>             Some(childExprs.last.convertToGpu())
>           } else {
>             None
>           }
>           GpuCaseWhen(branches, elseValue)
>         }
1665a2085,2088
>         override def convertToGpu(): GpuExpression = {
>           val Seq(boolExpr, trueExpr, falseExpr) = childExprs.map(_.convertToGpu())
>           GpuIf(boolExpr, trueExpr, falseExpr)
>         }
1673a2097,2098
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuPow(lhs, rhs)
1684a2110,2129
>         // Division of Decimal types is a little odd. To work around some issues with
>         // what Spark does the tagging/checks are in CheckOverflow instead of here.
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           a.dataType match {
>             case _: DecimalType =>
>               throw new IllegalStateException("Internal Error: Decimal Divide operations " +
>                   "should be converted to the GPU in the CheckOverflow rule")
>             case _ =>
>               GpuDivide(lhs, rhs)
>           }
>       }),
>     expr[IntegralDivide](
>       "Division with a integer result",
>       ExprChecks.binaryProject(
>         TypeSig.LONG, TypeSig.LONG,
>         ("lhs", TypeSig.LONG + TypeSig.DECIMAL_128, TypeSig.LONG + TypeSig.DECIMAL_128),
>         ("rhs", TypeSig.LONG + TypeSig.DECIMAL_128, TypeSig.LONG + TypeSig.DECIMAL_128)),
>       (a, conf, p, r) => new BinaryExprMeta[IntegralDivide](a, conf, p, r) {
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuIntegralDivide(lhs, rhs)
1686d2130
<     // IntegralDivide is not supported in spark 2.x
1693a2138,2139
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuRemainder(lhs, rhs)
1708c2154,2155
<         // No filter parameter in 2.x
---
>         private val filter: Option[BaseExprMeta[_]] =
>           a.filter.map(GpuOverrides.wrapExpr(_, conf, Some(this)))
1712c2159,2174
<           childrenExprMeta
---
>           childrenExprMeta ++ filter.toSeq
> 
>         override def convertToGpu(): GpuExpression = {
>           // handle the case AggregateExpression has the resultIds parameter where its
>           // Seq[ExprIds] instead of single ExprId.
>           val resultId = try {
>             val resultMethod = a.getClass.getMethod("resultId")
>             resultMethod.invoke(a).asInstanceOf[ExprId]
>           } catch {
>             case _: Exception =>
>               val resultMethod = a.getClass.getMethod("resultIds")
>               resultMethod.invoke(a).asInstanceOf[Seq[ExprId]].head
>           }
>           GpuAggregateExpression(childExprs.head.convertToGpu().asInstanceOf[GpuAggregateFunction],
>             a.mode, a.isDistinct, filter.map(_.convertToGpu()), resultId)
>         }
1735a2198,2200
>         // One of the few expressions that are not replaced with a GPU version
>         override def convertToGpu(): Expression =
>           sortOrder.withNewChildren(childExprs.map(_.convertToGpu()))
1757a2223,2229
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression = {
>           val Seq(pivotColumn, valueColumn) = childExprs
>           GpuPivotFirst(pivotColumn, valueColumn, pivot.pivotColumnValues)
>         }
> 
>         // Pivot does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
1770a2243,2244
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuCount(childExprs)
1791a2266,2270
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuMax(childExprs.head)
> 
>         // Max does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
1812a2292,2296
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuMin(childExprs.head)
> 
>         // Min does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
1825a2310,2330
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuSum(childExprs.head, a.dataType)
>       }),
>     expr[NthValue](
>       "nth window operator",
>       ExprChecks.windowOnly(
>         (TypeSig.STRUCT + TypeSig.ARRAY + TypeSig.MAP +
>             TypeSig.commonCudfTypes + TypeSig.NULL + TypeSig.DECIMAL_128).nested(),
>         TypeSig.all,
>         Seq(ParamCheck("input",
>           (TypeSig.STRUCT + TypeSig.ARRAY + TypeSig.MAP +
>               TypeSig.commonCudfTypes + TypeSig.NULL + TypeSig.DECIMAL_128).nested(),
>           TypeSig.all),
>           ParamCheck("offset", TypeSig.lit(TypeEnum.INT), TypeSig.lit(TypeEnum.INT)))
>       ),
>       (a, conf, p, r) => new AggExprMeta[NthValue](a, conf, p, r) {
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuNthValue(childExprs.head, a.offset, a.ignoreNulls)
> 
>         // nth does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
1827d2331
<     // Spark 2.x doesn't have NthValue
1839a2344,2348
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuFirst(childExprs.head, a.ignoreNulls)
> 
>         // First does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
1842c2351
<       "last aggregate operator",
---
>     "last aggregate operator",
1852a2362,2366
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuLast(childExprs.head, a.ignoreNulls)
> 
>         // Last does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
1871a2386,2387
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuBRound(lhs, rhs, a.dataType)
1890a2407,2408
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuRound(lhs, rhs, a.dataType)
1912a2431,2435
> 
>         override def convertToGpu(): GpuExpression =
>           GpuPythonUDF(a.name, a.func, a.dataType,
>             childExprs.map(_.convertToGpu()),
>             a.evalType, a.udfDeterministic, a.resultId)
1921a2445
>         override def convertToGpu(child: Expression): GpuExpression = GpuRand(child)
1926a2451
>         override def convertToGpu(): GpuExpression = GpuSparkPartitionID()
1931a2457
>         override def convertToGpu(): GpuExpression = GpuMonotonicallyIncreasingID()
1936a2463
>         override def convertToGpu(): GpuExpression = GpuInputFileName()
1941a2469
>         override def convertToGpu(): GpuExpression = GpuInputFileBlockStart()
1946a2475
>         override def convertToGpu(): GpuExpression = GpuInputFileBlockLength()
1952a2482
>         override def convertToGpu(child: Expression): GpuExpression = GpuMd5(child)
1957a2488
>         override def convertToGpu(child: Expression): GpuExpression = GpuUpper(child)
1963a2495
>         override def convertToGpu(child: Expression): GpuExpression = GpuLower(child)
1980a2513,2517
>         override def convertToGpu(
>             str: Expression,
>             width: Expression,
>             pad: Expression): GpuExpression =
>           GpuStringLPad(str, width, pad)
1996a2534,2538
>         override def convertToGpu(
>             str: Expression,
>             width: Expression,
>             pad: Expression): GpuExpression =
>           GpuStringRPad(str, width, pad)
2017a2560,2561
>         override def convertToGpu(arr: Expression): GpuExpression =
>           GpuGetStructField(arr, expr.ordinal, expr.name)
2029a2574,2575
>         override def convertToGpu(arr: Expression, ordinal: Expression): GpuExpression =
>           GpuGetArrayItem(arr, ordinal, in.failOnError)
2047a2594,2595
>         override def convertToGpu(map: Expression, key: Expression): GpuExpression =
>           GpuGetMapValue(map, key, in.failOnError)
2051c2599
<         "Returns value for the given key in value if column is map",
---
>         "Returns value for the given key in value if column is map.",
2096a2645,2647
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           GpuElementAt(lhs, rhs, failOnError = in.failOnError)
>         }
2107a2659,2660
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuMapKeys(child)
2118a2672,2687
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuMapValues(child)
>       }),
>     expr[MapEntries](
>       "Returns an unordered array of all entries in the given map",
>       ExprChecks.unaryProject(
>         // Technically the return type is an array of struct, but we cannot really express that
>         TypeSig.ARRAY.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL +
>             TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP + TypeSig.BINARY),
>         TypeSig.ARRAY.nested(TypeSig.all),
>         TypeSig.MAP.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL +
>             TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP + TypeSig.BINARY),
>         TypeSig.MAP.nested(TypeSig.all)),
>       (in, conf, p, r) => new UnaryExprMeta[MapEntries](in, conf, p, r) {
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuMapEntries(child)
2120d2688
<     // MapEntries is not supported in spark 2.x
2135a2704,2705
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuArrayMin(child)
2144a2715,2716
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuArrayMax(child)
2155a2728,2729
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuArrayRepeat(lhs, rhs)
2161a2736,2737
>         override def convertToGpu(): GpuExpression =
>           GpuCreateNamedStruct(childExprs.map(_.convertToGpu()))
2177a2754,2755
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuArrayContains(lhs, rhs)
2190c2768,2772
<       }),
---
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           GpuSortArray(lhs, rhs)
>         }
>       }
>     ),
2215a2798,2799
>         override def convertToGpu(): GpuExpression =
>           GpuCreateArray(childExprs.map(_.convertToGpu()), wrapped.useStringTypeWhenEmpty)
2231a2816,2822
>         override def convertToGpu(): GpuExpression = {
>           val func = childExprs.head
>           val args = childExprs.tail
>           GpuLambdaFunction(func.convertToGpu(),
>             args.map(_.convertToGpu().asInstanceOf[NamedExpression]),
>             in.hidden)
>         }
2239a2831,2833
>         override def convertToGpu(): GpuExpression = {
>           GpuNamedLambdaVariable(in.name, in.dataType, in.nullable, in.exprId)
>         }
2256a2851,2853
>         override def convertToGpu(): GpuExpression = {
>           GpuArrayTransform(childExprs.head.convertToGpu(), childExprs(1).convertToGpu())
>         }
2267a2865,2871
>         override def convertToGpu(): GpuExpression = {
>           GpuArrayExists(
>             childExprs.head.convertToGpu(),
>             childExprs(1).convertToGpu(),
>             SQLConf.get.getConf(SQLConf.LEGACY_ARRAY_EXISTS_FOLLOWS_THREE_VALUED_LOGIC)
>           )
>         }
2281a2886,2888
>         override def convertToGpu(): GpuExpression = {
>           GpuArraysZip(childExprs.map(_.convertToGpu()))
>         }
2295a2903,2905
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           GpuArrayExcept(lhs, rhs)
>         }
2314a2925,2948
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           GpuArrayIntersect(lhs, rhs)
>         }
>       }
>     ).incompat("the GPU implementation treats -0.0 and 0.0 as equal, but the CPU " +
>         "implementation currently does not (see SPARK-39845). Also, Apache Spark " +
>         "3.1.3 fixed issue SPARK-36741 where NaNs in these set like operators were " +
>         "not treated as being equal. We have chosen to break with compatibility for " +
>         "the older versions of Spark in this instance and handle NaNs the same as 3.1.3+"),
>     expr[ArrayUnion](
>       "Returns an array of the elements in the union of array1 and array2, without duplicates.",
>       ExprChecks.binaryProject(
>         TypeSig.ARRAY.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL),
>         TypeSig.ARRAY.nested(TypeSig.all),
>         ("array1",
>             TypeSig.ARRAY.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL),
>             TypeSig.ARRAY.nested(TypeSig.all)),
>         ("array2",
>             TypeSig.ARRAY.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL),
>             TypeSig.ARRAY.nested(TypeSig.all))),
>       (in, conf, p, r) => new BinaryExprMeta[ArrayUnion](in, conf, p, r) {
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           GpuArrayUnion(lhs, rhs)
>         }
2321d2954
<     // ArrayUnion is not supported in Spark 2.x
2333a2967,2969
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression = {
>           GpuArraysOverlap(lhs, rhs)
>         }
2340,2342c2976,3037
<     // TransformKeys is not supported in Spark 2.x
<     // TransformValues is not supported in Spark 2.x
<     // spark 2.x doesn't have MapFilter
---
>     expr[TransformKeys](
>       "Transform keys in a map using a transform function",
>       ExprChecks.projectOnly(TypeSig.MAP.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 +
>           TypeSig.NULL + TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP),
>         TypeSig.MAP.nested(TypeSig.all),
>         Seq(
>           ParamCheck("argument",
>             TypeSig.MAP.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL +
>                 TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP),
>             TypeSig.MAP.nested(TypeSig.all)),
>           ParamCheck("function",
>             // We need to be able to check for duplicate keys (equality)
>             TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL,
>             TypeSig.all - TypeSig.MAP.nested()))),
>       (in, conf, p, r) => new ExprMeta[TransformKeys](in, conf, p, r) {
>         override def tagExprForGpu(): Unit = {
>           SQLConf.get.getConf(SQLConf.MAP_KEY_DEDUP_POLICY).toUpperCase match {
>             case "EXCEPTION"| "LAST_WIN" => // Good we can support this
>             case other =>
>               willNotWorkOnGpu(s"$other is not supported for config setting" +
>                   s" ${SQLConf.MAP_KEY_DEDUP_POLICY.key}")
>           }
>         }
>         override def convertToGpu(): GpuExpression = {
>           GpuTransformKeys(childExprs.head.convertToGpu(), childExprs(1).convertToGpu())
>         }
>       }),
>     expr[TransformValues](
>       "Transform values in a map using a transform function",
>       ExprChecks.projectOnly(TypeSig.MAP.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 +
>           TypeSig.NULL + TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP),
>         TypeSig.MAP.nested(TypeSig.all),
>         Seq(
>           ParamCheck("argument",
>             TypeSig.MAP.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL +
>                 TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP),
>             TypeSig.MAP.nested(TypeSig.all)),
>           ParamCheck("function",
>             (TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL +
>                 TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP).nested(),
>             TypeSig.all))),
>       (in, conf, p, r) => new ExprMeta[TransformValues](in, conf, p, r) {
>         override def convertToGpu(): GpuExpression = {
>           GpuTransformValues(childExprs.head.convertToGpu(), childExprs(1).convertToGpu())
>         }
>       }),
>     expr[MapFilter](
>       "Filters entries in a map using the function",
>       ExprChecks.projectOnly(TypeSig.MAP.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 +
>           TypeSig.NULL + TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP),
>         TypeSig.MAP.nested(TypeSig.all),
>         Seq(
>           ParamCheck("argument",
>             TypeSig.MAP.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL +
>                 TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.MAP),
>             TypeSig.MAP.nested(TypeSig.all)),
>           ParamCheck("function", TypeSig.BOOLEAN, TypeSig.BOOLEAN))),
>       (in, conf, p, r) => new ExprMeta[MapFilter](in, conf, p, r) {
>         override def convertToGpu(): GpuExpression = {
>           GpuMapFilter(childExprs.head.convertToGpu(), childExprs(1).convertToGpu())
>         }
>       }),
2349a3045,3049
>         override def convertToGpu(
>             val0: Expression,
>             val1: Expression,
>             val2: Expression): GpuExpression =
>           GpuStringLocate(val0, val1, val2)
2357a3058,3062
>         override def convertToGpu(
>             column: Expression,
>             position: Expression,
>             length: Expression): GpuExpression =
>           GpuSubstring(column, position, length)
2373a3079,3081
>         override def convertToGpu(
>             input: Expression,
>             repeatTimes: Expression): GpuExpression = GpuStringRepeat(input, repeatTimes)
2381a3090,3094
>         override def convertToGpu(
>             column: Expression,
>             target: Expression,
>             replace: Expression): GpuExpression =
>           GpuStringReplace(column, target, replace)
2389a3103,3106
>         override def convertToGpu(
>             column: Expression,
>             target: Option[Expression] = None): GpuExpression =
>           GpuStringTrim(column, target)
2398a3116,3119
>           override def convertToGpu(
>             column: Expression,
>             target: Option[Expression] = None): GpuExpression =
>             GpuStringTrimLeft(column, target)
2407a3129,3132
>           override def convertToGpu(
>               column: Expression,
>               target: Option[Expression] = None): GpuExpression =
>             GpuStringTrimRight(column, target)
2414a3140,3141
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuStartsWith(lhs, rhs)
2421a3149,3150
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuEndsWith(lhs, rhs)
2434a3164
>         override def convertToGpu(child: Seq[Expression]): GpuExpression = GpuConcat(child)
2445a3176
>         override def convertToGpu(child: Seq[Expression]): GpuExpression = GpuMapConcat(child)
2462a3194,3195
>         override final def convertToGpu(): GpuExpression =
>           GpuConcatWs(childExprs.map(_.convertToGpu()))
2472a3206,3207
>         def convertToGpu(): GpuExpression =
>           GpuMurmur3Hash(childExprs.map(_.convertToGpu()), a.seed)
2479a3215,3216
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuContains(lhs, rhs)
2486a3224,3225
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuLike(lhs, rhs, a.escapeChar)
2512c3251,3258
<     // RegExpExtractAll is not supported in Spark 2.x
---
>     expr[RegExpExtractAll](
>       "Extract all strings matching a regular expression corresponding to the regex group index",
>       ExprChecks.projectOnly(TypeSig.ARRAY.nested(TypeSig.STRING),
>         TypeSig.ARRAY.nested(TypeSig.STRING),
>         Seq(ParamCheck("str", TypeSig.STRING, TypeSig.STRING),
>           ParamCheck("regexp", TypeSig.lit(TypeEnum.STRING), TypeSig.STRING),
>           ParamCheck("idx", TypeSig.lit(TypeEnum.INT), TypeSig.INT))),
>       (a, conf, p, r) => new GpuRegExpExtractAllMeta(a, conf, p, r)),
2517a3264
>         override def convertToGpu(child: Expression): GpuExpression = GpuLength(child)
2525a3273,3274
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuSize(child, a.legacySizeOfNull)
2532a3282
>         override def convertToGpu(child: Expression): GpuExpression = GpuUnscaledValue(child)
2538a3289,3290
>         override def convertToGpu(child: Expression): GpuExpression =
>           GpuMakeDecimal(child, a.precision, a.scale, a.nullOnOverflow)
2552a3305
>         override def convertToGpu(): GpuExpression = GpuExplode(childExprs.head.convertToGpu())
2566a3320
>         override def convertToGpu(): GpuExpression = GpuPosExplode(childExprs.head.convertToGpu())
2579,2580c3333,3413
<      }),
<     // spark 2.x CollectList and CollectSet use TypedImperative which isn't in 2.x
---
>         override def convertToGpu(childExpr: Seq[Expression]): GpuExpression =
>           GpuReplicateRows(childExpr)
>       }),
>     expr[CollectList](
>       "Collect a list of non-unique elements, not supported in reduction",
>       ExprChecks.fullAgg(
>         TypeSig.ARRAY.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 +
>             TypeSig.NULL + TypeSig.STRUCT + TypeSig.ARRAY + TypeSig.MAP),
>         TypeSig.ARRAY.nested(TypeSig.all),
>         Seq(ParamCheck("input",
>           (TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 +
>               TypeSig.NULL + TypeSig.STRUCT + TypeSig.ARRAY + TypeSig.MAP).nested(),
>           TypeSig.all))),
>       (c, conf, p, r) => new TypedImperativeAggExprMeta[CollectList](c, conf, p, r) {
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuCollectList(childExprs.head, c.mutableAggBufferOffset, c.inputAggBufferOffset)
> 
>         override def aggBufferAttribute: AttributeReference = {
>           val aggBuffer = c.aggBufferAttributes.head
>           aggBuffer.copy(dataType = c.dataType)(aggBuffer.exprId, aggBuffer.qualifier)
>         }
> 
>         override def createCpuToGpuBufferConverter(): CpuToGpuAggregateBufferConverter =
>           new CpuToGpuCollectBufferConverter(c.child.dataType)
> 
>         override def createGpuToCpuBufferConverter(): GpuToCpuAggregateBufferConverter =
>           new GpuToCpuCollectBufferConverter()
> 
>         override val supportBufferConversion: Boolean = true
> 
>         // Last does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
>       }),
>     expr[CollectSet](
>       "Collect a set of unique elements, not supported in reduction",
>       ExprChecks.fullAgg(
>         TypeSig.ARRAY.nested(TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 +
>             TypeSig.NULL + TypeSig.STRUCT + TypeSig.ARRAY),
>         TypeSig.ARRAY.nested(TypeSig.all),
>         Seq(ParamCheck("input",
>           (TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 +
>               TypeSig.NULL + 
>               TypeSig.STRUCT +
>               TypeSig.ARRAY).nested(),
>           TypeSig.all))),
>       (c, conf, p, r) => new TypedImperativeAggExprMeta[CollectSet](c, conf, p, r) {
> 
>         private def isNestedArrayType(dt: DataType): Boolean = {
>           dt match {
>             case StructType(fields) =>
>               fields.exists { field =>
>                 field.dataType match {
>                   case sdt: StructType => isNestedArrayType(sdt)
>                   case _: ArrayType => true
>                   case _ => false
>                 }
>               }
>             case ArrayType(et, _) => et.isInstanceOf[ArrayType] || et.isInstanceOf[StructType]
>             case _ => false
>           }
>         }
> 
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuCollectSet(childExprs.head, c.mutableAggBufferOffset, c.inputAggBufferOffset)
> 
>         override def aggBufferAttribute: AttributeReference = {
>           val aggBuffer = c.aggBufferAttributes.head
>           aggBuffer.copy(dataType = c.dataType)(aggBuffer.exprId, aggBuffer.qualifier)
>         }
> 
>         override def createCpuToGpuBufferConverter(): CpuToGpuAggregateBufferConverter =
>           new CpuToGpuCollectBufferConverter(c.child.dataType)
> 
>         override def createGpuToCpuBufferConverter(): GpuToCpuAggregateBufferConverter =
>           new GpuToCpuCollectBufferConverter()
> 
>         override val supportBufferConversion: Boolean = true
> 
>         // Last does not overflow, so it doesn't need the ANSI check
>         override val needsAnsiCheck: Boolean = false
>       }),
2586a3420,3423
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression = {
>           val legacyStatisticalAggregate = SQLConf.get.legacyStatisticalAggregate
>           GpuStddevPop(childExprs.head, !legacyStatisticalAggregate)
>         }
2594a3432,3435
>           override def convertToGpu(childExprs: Seq[Expression]): GpuExpression = {
>             val legacyStatisticalAggregate = SQLConf.get.legacyStatisticalAggregate
>             GpuStddevSamp(childExprs.head, !legacyStatisticalAggregate)
>           }
2601a3443,3446
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression = {
>           val legacyStatisticalAggregate = SQLConf.get.legacyStatisticalAggregate
>           GpuVariancePop(childExprs.head, !legacyStatisticalAggregate)
>         }
2608a3454,3457
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression = {
>           val legacyStatisticalAggregate = SQLConf.get.legacyStatisticalAggregate
>           GpuVarianceSamp(childExprs.head, !legacyStatisticalAggregate)
>         }
2648a3498,3503
> 
>         override def convertToGpu(childExprs: Seq[Expression]): GpuExpression =
>           GpuApproximatePercentile(childExprs.head,
>               childExprs(1).asInstanceOf[GpuLiteral],
>               childExprs(2).asInstanceOf[GpuLiteral])
> 
2663a3519,3520
>         override def convertToGpu(lhs: Expression, rhs: Expression): GpuExpression =
>           GpuGetJsonObject(lhs, rhs)
2675c3532,3534
<       }),
---
>           override def convertToGpu(): GpuExpression = GpuScalarSubquery(a.plan, a.exprId)
>         }
>     ),
2680c3539,3541
<       }),
---
>         override def convertToGpu(): GpuExpression = GpuCreateMap(childExprs.map(_.convertToGpu()))
>       }
>     ),
2698a3560
>         override def convertToGpu(child: Expression): GpuExpression = GpuBitLength(child)
2705a3568
>         override def convertToGpu(child: Expression): GpuExpression = GpuOctetLength(child)
2719,2721c3582,3592
<       )
<     // Spark 2.x doesn't have RaiseError or ansicast
<   ).map(r => (r.getClassFor.asSubclass(classOf[Expression]), r)).toMap
---
>     ),
>     expr[RaiseError](
>       "Throw an exception",
>       ExprChecks.unaryProject(
>         TypeSig.NULL, TypeSig.NULL,
>         TypeSig.STRING, TypeSig.STRING),
>       (a, conf, p, r) => new UnaryExprMeta[RaiseError](a, conf, p, r) {
>         override def convertToGpu(child: Expression): GpuExpression = GpuRaiseError(child)
>       }),
>     SparkShimImpl.ansiCastRule
>   ).collect { case r if r != null => (r.getClassFor.asSubclass(classOf[Expression]), r)}.toMap
2726c3597,3644
<         ShimGpuOverrides.shimExpressions
---
>         SparkShimImpl.getExprs
> 
>   def wrapScan[INPUT <: Scan](
>       scan: INPUT,
>       conf: RapidsConf,
>       parent: Option[RapidsMeta[_, _, _]]): ScanMeta[INPUT] =
>     scans.get(scan.getClass)
>       .map(r => r.wrap(scan, conf, parent, r).asInstanceOf[ScanMeta[INPUT]])
>       .getOrElse(new RuleNotFoundScanMeta(scan, conf, parent))
> 
>   val commonScans: Map[Class[_ <: Scan], ScanRule[_ <: Scan]] = Seq(
>     GpuOverrides.scan[CSVScan](
>       "CSV parsing",
>       (a, conf, p, r) => new ScanMeta[CSVScan](a, conf, p, r) {
>         override def tagSelfForGpu(): Unit = GpuCSVScan.tagSupport(this)
> 
>         override def convertToGpu(): Scan =
>           GpuCSVScan(a.sparkSession,
>             a.fileIndex,
>             a.dataSchema,
>             a.readDataSchema,
>             a.readPartitionSchema,
>             a.options,
>             a.partitionFilters,
>             a.dataFilters,
>             conf.maxReadBatchSizeRows,
>             conf.maxReadBatchSizeBytes)
>       }),
>     GpuOverrides.scan[JsonScan](
>       "Json parsing",
>       (a, conf, p, r) => new ScanMeta[JsonScan](a, conf, p, r) {
>         override def tagSelfForGpu(): Unit = GpuJsonScan.tagSupport(this)
> 
>         override def convertToGpu(): Scan =
>           GpuJsonScan(a.sparkSession,
>             a.fileIndex,
>             a.dataSchema,
>             a.readDataSchema,
>             a.readPartitionSchema,
>             a.options,
>             a.partitionFilters,
>             a.dataFilters,
>             conf.maxReadBatchSizeRows,
>             conf.maxReadBatchSizeBytes)
>       })).map(r => (r.getClassFor.asSubclass(classOf[Scan]), r)).toMap
> 
>   val scans: Map[Class[_ <: Scan], ScanRule[_ <: Scan]] =
>     commonScans ++ SparkShimImpl.getScans ++ ExternalSource.getScans
2731c3649
<       parent: Option[RapidsMeta[_, _]]): PartMeta[INPUT] =
---
>       parent: Option[RapidsMeta[_, _, _]]): PartMeta[INPUT] =
2746a3665,3666
>         override def convertToGpu(): GpuPartitioning =
>           GpuHashPartitioning(childExprs.map(_.convertToGpu()), hp.numPartitions)
2756a3677,3684
>         override def convertToGpu(): GpuPartitioning = {
>           if (rp.numPartitions > 1) {
>             val gpuOrdering = childExprs.map(_.convertToGpu()).asInstanceOf[Seq[SortOrder]]
>             GpuRangePartitioning(gpuOrdering, rp.numPartitions)
>           } else {
>             GpuSinglePartitioning
>           }
>         }
2761a3690,3692
>         override def convertToGpu(): GpuPartitioning = {
>           GpuRoundRobinPartitioning(rrp.numPartitions)
>         }
2766a3698
>         override def convertToGpu(): GpuPartitioning = GpuSinglePartitioning
2773c3705
<       parent: Option[RapidsMeta[_, _]]): DataWritingCommandMeta[INPUT] =
---
>       parent: Option[RapidsMeta[_, _, _]]): DataWritingCommandMeta[INPUT] =
2795c3727
<       parent: Option[RapidsMeta[_, _]]): SparkPlanMeta[INPUT]  =
---
>       parent: Option[RapidsMeta[_, _, _]]): SparkPlanMeta[INPUT]  =
2819c3751,3769
<       (range, conf, p, r) => new SparkPlanMeta[RangeExec](range, conf, p, r) {
---
>       (range, conf, p, r) => {
>         new SparkPlanMeta[RangeExec](range, conf, p, r) {
>           override def convertToGpu(): GpuExec =
>             GpuRangeExec(range.start, range.end, range.step, range.numSlices, range.output,
>               conf.gpuTargetBatchSizeBytes)
>         }
>       }),
>     exec[BatchScanExec](
>       "The backend for most file input",
>       ExecChecks(
>         (TypeSig.commonCudfTypes + TypeSig.STRUCT + TypeSig.MAP + TypeSig.ARRAY +
>             TypeSig.DECIMAL_128 + TypeSig.BINARY).nested(),
>         TypeSig.all),
>       (p, conf, parent, r) => new SparkPlanMeta[BatchScanExec](p, conf, parent, r) {
>         override val childScans: scala.Seq[ScanMeta[_]] =
>           Seq(GpuOverrides.wrapScan(p.scan, conf, Some(this)))
> 
>         override def convertToGpu(): GpuExec =
>           GpuBatchScanExec(p.output, childScans.head.convertToGpu())
2826a3777,3778
>         override def convertToGpu(): GpuExec =
>           GpuCoalesceExec(coalesce.numPartitions, childPlans.head.convertIfNeeded())
2834a3787
>           TypeSig.BINARY.withPsNote(TypeEnum.BINARY, "Only supported for Parquet") +
2840a3794,3796
>         override def convertToGpu(): GpuExec =
>           GpuDataWritingCommandExec(childDataWriteCmds.head.convertToGpu(),
>             childPlans.head.convertIfNeeded())
2855a3812,3836
>           override def convertToGpu(): GpuExec = {
>             // To avoid metrics confusion we split a single stage up into multiple parts but only
>             // if there are multiple partitions to make it worth doing.
>             val so = sortOrder.map(_.convertToGpu().asInstanceOf[SortOrder])
>             if (takeExec.child.outputPartitioning.numPartitions == 1) {
>               GpuTopN(takeExec.limit, so,
>                 projectList.map(_.convertToGpu().asInstanceOf[NamedExpression]),
>                 childPlans.head.convertIfNeeded())(takeExec.sortOrder)
>             } else {
>               GpuTopN(
>                 takeExec.limit,
>                 so,
>                 projectList.map(_.convertToGpu().asInstanceOf[NamedExpression]),
>                 GpuShuffleExchangeExec(
>                   GpuSinglePartitioning,
>                   GpuTopN(
>                     takeExec.limit,
>                     so,
>                     takeExec.child.output,
>                     childPlans.head.convertIfNeeded())(takeExec.sortOrder),
>                   ENSURE_REQUIREMENTS
>                 )(SinglePartition)
>               )(takeExec.sortOrder)
>             }
>           }
2863a3845,3846
>           override def convertToGpu(): GpuExec =
>             GpuLocalLimitExec(localLimitExec.limit, childPlans.head.convertIfNeeded())
2871a3855,3856
>           override def convertToGpu(): GpuExec =
>             GpuGlobalLimitExec(globalLimitExec.limit, childPlans.head.convertIfNeeded(), 0)
2878,2881c3863
<       (collectLimitExec, conf, p, r) =>
<         new SparkPlanMeta[CollectLimitExec](collectLimitExec, conf, p, r) {
<           override val childParts: scala.Seq[PartMeta[_]] =
<             Seq(GpuOverrides.wrapPart(collectLimitExec.outputPartitioning, conf, Some(this)))})
---
>       (collectLimitExec, conf, p, r) => new GpuCollectLimitMeta(collectLimitExec, conf, p, r))
2890a3873,3874
>         override def convertToGpu(): GpuExec =
>           GpuFilterExec(childExprs.head.convertToGpu(), childPlans.head.convertIfNeeded())
2912a3897,3898
>         override def convertToGpu(): GpuExec =
>           GpuUnionExec(childPlans.map(_.convertIfNeeded()))
2943a3930,3940
>         override def convertToGpu(): GpuExec = {
>           val Seq(left, right) = childPlans.map(_.convertIfNeeded())
>           val joinExec = GpuCartesianProductExec(
>             left,
>             right,
>             None,
>             conf.gpuTargetBatchSizeBytes)
>           // The GPU does not yet support conditional joins, so conditions are implemented
>           // as a filter after the join when possible.
>           condition.map(c => GpuFilterExec(c.convertToGpu(), joinExec)).getOrElse(joinExec)
>         }
2956a3954,3969
>     exec[ObjectHashAggregateExec](
>       "The backend for hash based aggregations supporting TypedImperativeAggregate functions",
>       ExecChecks(
>         // note that binary input is allowed here but there are additional checks later on to
>         // check that we have can support binary in the context of aggregate buffer conversions
>         (TypeSig.commonCudfTypes + TypeSig.NULL + TypeSig.DECIMAL_128 +
>           TypeSig.MAP + TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.BINARY)
>             .nested()
>             .withPsNote(TypeEnum.BINARY, "only allowed when aggregate buffers can be " +
>               "converted between CPU and GPU")
>             .withPsNote(Seq(TypeEnum.ARRAY, TypeEnum.MAP),
>               "not allowed for grouping expressions")
>             .withPsNote(TypeEnum.STRUCT,
>               "not allowed for grouping expressions if containing Array or Map as child"),
>         TypeSig.all),
>       (agg, conf, p, r) => new GpuObjectHashAggregateExecMeta(agg, conf, p, r)),
2963,2964d3975
<       // SPARK 2.x we can't check for the TypedImperativeAggregate properly so
<       // map/array/struct left off
2967c3978
<             TypeSig.MAP + TypeSig.BINARY)
---
>             TypeSig.MAP + TypeSig.ARRAY + TypeSig.STRUCT + TypeSig.BINARY)
2969d3979
<             .withPsNote(TypeEnum.MAP, "not allowed for grouping expressions")
2971c3981,3985
<               "converted between CPU and GPU"),
---
>               "converted between CPU and GPU")
>             .withPsNote(Seq(TypeEnum.ARRAY, TypeEnum.MAP),
>               "not allowed for grouping expressions")
>             .withPsNote(TypeEnum.STRUCT,
>               "not allowed for grouping expressions if containing Array or Map as child"),
2974,2975d3987
<     // SPARK 2.x we can't check for the TypedImperativeAggregate properly so don't say we do the
<     // ObjectHashAggregate
3012c4024
<       (sample, conf, p, r) => new GpuSampleExecMeta(sample, conf, p, r) {}
---
>       (sample, conf, p, r) => new GpuSampleExecMeta(sample, conf, p, r)
3013a4026,4031
>     exec[SubqueryBroadcastExec](
>       "Plan to collect and transform the broadcast key values",
>       ExecChecks(TypeSig.all, TypeSig.all),
>       (s, conf, p, r) => new GpuSubqueryBroadcastMeta(s, conf, p, r)
>     ),
>     SparkShimImpl.aqeShuffleReaderExec,
3032c4050
<             e.output.map(GpuOverrides.wrapExpr(_, conf, Some(this)))
---
>             e.resultAttrs.map(GpuOverrides.wrapExpr(_, conf, Some(this)))
3037a4056,4061
> 
>           override def convertToGpu(): GpuExec =
>             GpuArrowEvalPythonExec(udfs.map(_.convertToGpu()).asInstanceOf[Seq[GpuPythonUDF]],
>               resultAttrs.map(_.convertToGpu()).asInstanceOf[Seq[Attribute]],
>               childPlans.head.convertIfNeeded(),
>               e.evalType)
3038a4063,4068
>     exec[FlatMapCoGroupsInPandasExec](
>       "The backend for CoGrouped Aggregation Pandas UDF, it runs on CPU itself now but supports" +
>         " scheduling GPU resources for the Python process when enabled",
>       ExecChecks.hiddenHack(),
>       (flatCoPy, conf, p, r) => new GpuFlatMapCoGroupsInPandasExecMeta(flatCoPy, conf, p, r))
>         .disabledByDefault("Performance is not ideal now"),
3044a4075,4081
>     exec[MapInPandasExec](
>       "The backend for Map Pandas Iterator UDF. Accelerates the data transfer between the" +
>         " Java process and the Python process. It also supports scheduling GPU resources" +
>         " for the Python process when enabled.",
>       ExecChecks((TypeSig.commonCudfTypes + TypeSig.ARRAY + TypeSig.STRUCT).nested(),
>         TypeSig.all),
>       (mapPy, conf, p, r) => new GpuMapInPandasExecMeta(mapPy, conf, p, r)),
3050,3052c4087,4108
<     // SparkShimImpl.aqeShuffleReaderExec,
<     // SparkShimImpl.neverReplaceShowCurrentNamespaceCommand,
<     neverReplaceExec[ExecutedCommandExec]("Table metadata operation")
---
>     neverReplaceExec[AlterNamespaceSetPropertiesExec]("Namespace metadata operation"),
>     neverReplaceExec[CreateNamespaceExec]("Namespace metadata operation"),
>     neverReplaceExec[DescribeNamespaceExec]("Namespace metadata operation"),
>     neverReplaceExec[DropNamespaceExec]("Namespace metadata operation"),
>     neverReplaceExec[SetCatalogAndNamespaceExec]("Namespace metadata operation"),
>     SparkShimImpl.neverReplaceShowCurrentNamespaceCommand,
>     neverReplaceExec[ShowNamespacesExec]("Namespace metadata operation"),
>     neverReplaceExec[ExecutedCommandExec]("Table metadata operation"),
>     neverReplaceExec[AlterTableExec]("Table metadata operation"),
>     neverReplaceExec[CreateTableExec]("Table metadata operation"),
>     neverReplaceExec[DeleteFromTableExec]("Table metadata operation"),
>     neverReplaceExec[DescribeTableExec]("Table metadata operation"),
>     neverReplaceExec[DropTableExec]("Table metadata operation"),
>     neverReplaceExec[AtomicReplaceTableExec]("Table metadata operation"),
>     neverReplaceExec[RefreshTableExec]("Table metadata operation"),
>     neverReplaceExec[RenameTableExec]("Table metadata operation"),
>     neverReplaceExec[ReplaceTableExec]("Table metadata operation"),
>     neverReplaceExec[ShowTablePropertiesExec]("Table metadata operation"),
>     neverReplaceExec[ShowTablesExec]("Table metadata operation"),
>     neverReplaceExec[AdaptiveSparkPlanExec]("Wrapper for adaptive query plan"),
>     neverReplaceExec[BroadcastQueryStageExec]("Broadcast query stage"),
>     neverReplaceExec[ShuffleQueryStageExec]("Shuffle query stage")
3056c4112
<     commonExecs ++ ShimGpuOverrides.shimExecs
---
>     commonExecs ++ SparkShimImpl.getExecs
3059,3061c4115
<     // val key = SQLConf.LEGACY_TIME_PARSER_POLICY.key
<     val key = "2xgone"
<     val policy = SQLConf.get.getConfString(key, "EXCEPTION")
---
>     val policy = SQLConf.get.getConfString(SQLConf.LEGACY_TIME_PARSER_POLICY.key, "EXCEPTION")
3068a4123,4127
>   val preRowToColProjection = TreeNodeTag[Seq[NamedExpression]]("rapids.gpu.preRowToColProcessing")
> 
>   val postColToRowProjection = TreeNodeTag[Seq[NamedExpression]](
>     "rapids.gpu.postColToRowProcessing")
> 
3074a4134,4141
>   private def doConvertPlan(wrap: SparkPlanMeta[SparkPlan], conf: RapidsConf,
>       optimizations: Seq[Optimization]): SparkPlan = {
>     val convertedPlan = wrap.convertIfNeeded()
>     val sparkPlan = addSortsIfNeeded(convertedPlan, conf)
>     GpuOverrides.listeners.foreach(_.optimizedPlan(wrap, sparkPlan, optimizations))
>     sparkPlan
>   }
> 
3077c4144,4192
<    Seq.empty
---
>     if (conf.optimizerEnabled) {
>       // we need to run these rules both before and after CBO because the cost
>       // is impacted by forcing operators onto CPU due to other rules that we have
>       wrap.runAfterTagRules()
>       val optimizer = try {
>         ShimLoader.newOptimizerClass(conf.optimizerClassName)
>       } catch {
>         case e: Exception =>
>           throw new RuntimeException(s"Failed to create optimizer ${conf.optimizerClassName}", e)
>       }
>       optimizer.optimize(conf, wrap)
>     } else {
>       Seq.empty
>     }
>   }
> 
>   private def addSortsIfNeeded(plan: SparkPlan, conf: RapidsConf): SparkPlan = {
>     plan.transformUp {
>       case operator: SparkPlan =>
>         ensureOrdering(operator, conf)
>     }
>   }
> 
>   // copied from Spark EnsureRequirements but only does the ordering checks and
>   // check to convert any SortExec added to GpuSortExec
>   private def ensureOrdering(operator: SparkPlan, conf: RapidsConf): SparkPlan = {
>     val requiredChildOrderings: Seq[Seq[SortOrder]] = operator.requiredChildOrdering
>     var children: Seq[SparkPlan] = operator.children
>     assert(requiredChildOrderings.length == children.length)
> 
>     // Now that we've performed any necessary shuffles, add sorts to guarantee output orderings:
>     children = children.zip(requiredChildOrderings).map { case (child, requiredOrdering) =>
>       // If child.outputOrdering already satisfies the requiredOrdering, we do not need to sort.
>       if (SortOrder.orderingSatisfies(child.outputOrdering, requiredOrdering)) {
>         child
>       } else {
>         val sort = SortExec(requiredOrdering, global = false, child = child)
>         // just specifically check Sort to see if we can change Sort to GPUSort
>         val sortMeta = new GpuSortMeta(sort, conf, None, new SortDataFromReplacementRule)
>         sortMeta.initReasons()
>         sortMeta.tagPlanForGpu()
>         if (sortMeta.canThisBeReplaced) {
>           sortMeta.convertToGpu()
>         } else {
>           sort
>         }
>       }
>     }
>     operator.withNewChildren(children)
3087,3088c4202,4208
<   // Only run the explain and don't actually convert or run on GPU.
<   def explainPotentialGpuPlan(df: DataFrame, explain: String = "ALL"): String = {
---
>   /**
>    * Only run the explain and don't actually convert or run on GPU.
>    * This gets the plan from the dataframe so it's after catalyst has run through all the
>    * rules to modify the plan. This means we have to try to undo some of the last rules
>    * to make it close to when the columnar rules would normally run on the plan.
>    */
>   def explainPotentialGpuPlan(df: DataFrame, explain: String): String = {
3114a4235,4255
>   /**
>    * Use explain mode on an active SQL plan as its processed through catalyst.
>    * This path is the same as being run through the plugin running on hosts with
>    * GPUs.
>    */
>   private def explainCatalystSQLPlan(updatedPlan: SparkPlan, conf: RapidsConf): Unit = {
>     // Since we set "NOT_ON_GPU" as the default value of spark.rapids.sql.explain, here we keep
>     // "ALL" as default value of "explainSetting", unless spark.rapids.sql.explain is changed
>     // by the user.
>     val explainSetting = if (conf.shouldExplain &&
>       conf.isConfExplicitlySet(RapidsConf.EXPLAIN.key)) {
>       conf.explain
>     } else {
>       "ALL"
>     }
>     val explainOutput = explainSinglePlan(updatedPlan, conf, explainSetting)
>     if (explainOutput.nonEmpty) {
>       logWarning(s"\n$explainOutput")
>     }
>   }
> 
3137c4278
<       // case c2r: ColumnarToRowExec => prepareExplainOnly(c2r.child)
---
>       case c2r: ColumnarToRowExec => prepareExplainOnly(c2r.child)
3139,3140c4280,4281
<       // case aqe: AdaptiveSparkPlanExec =>
<       //   prepareExplainOnly(SparkShimImpl.getAdaptiveInputPlan(aqe))
---
>       case aqe: AdaptiveSparkPlanExec =>
>         prepareExplainOnly(SparkShimImpl.getAdaptiveInputPlan(aqe))
3147,3151c4288,4370
< // Spark 2.x
< object GpuUserDefinedFunction {
<   // UDFs can support all types except UDT which does not have a clear columnar representation.
<   val udfTypeSig: TypeSig = (TypeSig.commonCudfTypes + TypeSig.DECIMAL_128 + TypeSig.NULL +
<       TypeSig.BINARY + TypeSig.CALENDAR + TypeSig.ARRAY + TypeSig.MAP + TypeSig.STRUCT).nested()
---
> /**
>  * Note, this class should not be referenced directly in source code.
>  * It should be loaded by reflection using ShimLoader.newInstanceOf, see ./docs/dev/shims.md
>  */
> protected class ExplainPlanImpl extends ExplainPlanBase {
>   override def explainPotentialGpuPlan(df: DataFrame, explain: String): String = {
>     GpuOverrides.explainPotentialGpuPlan(df, explain)
>   }
> }
> 
> // work around any GpuOverride failures
> object GpuOverrideUtil extends Logging {
>   def tryOverride(fn: SparkPlan => SparkPlan): SparkPlan => SparkPlan = { plan =>
>     val planOriginal = plan.clone()
>     val failOnError = TEST_CONF.get(plan.conf) || !SUPPRESS_PLANNING_FAILURE.get(plan.conf)
>     try {
>       fn(plan)
>     } catch {
>       case NonFatal(t) if !failOnError =>
>         logWarning("Failed to apply GPU overrides, falling back on the original plan: " + t, t)
>         planOriginal
>       case fatal: Throwable =>
>         logError("Encountered an exception applying GPU overrides " + fatal, fatal)
>         throw fatal
>     }
>   }
> }
> 
> /** Tag the initial plan when AQE is enabled */
> case class GpuQueryStagePrepOverrides() extends Rule[SparkPlan] with Logging {
>   override def apply(sparkPlan: SparkPlan): SparkPlan = GpuOverrideUtil.tryOverride { plan =>
>     // Note that we disregard the GPU plan returned here and instead rely on side effects of
>     // tagging the underlying SparkPlan.
>     GpuOverrides().applyWithContext(plan, Some("AQE Query Stage Prep"))
>     // return the original plan which is now modified as a side-effect of invoking GpuOverrides
>     plan
>   }(sparkPlan)
> }
> 
> case class GpuOverrides() extends Rule[SparkPlan] with Logging {
> 
>   // Spark calls this method once for the whole plan when AQE is off. When AQE is on, it
>   // gets called once for each query stage (where a query stage is an `Exchange`).
>   override def apply(sparkPlan: SparkPlan): SparkPlan = applyWithContext(sparkPlan, None)
> 
>   def applyWithContext(sparkPlan: SparkPlan, context: Option[String]): SparkPlan =
>       GpuOverrideUtil.tryOverride { plan =>
>     val conf = new RapidsConf(plan.conf)
>     if (conf.isSqlEnabled && conf.isSqlExecuteOnGPU) {
>       GpuOverrides.logDuration(conf.shouldExplain,
>         t => f"Plan conversion to the GPU took $t%.2f ms") {
>         val updatedPlan = updateForAdaptivePlan(plan, conf)
>         val newPlan = applyOverrides(updatedPlan, conf)
>         if (conf.logQueryTransformations) {
>           val logPrefix = context.map(str => s"[$str]").getOrElse("")
>           logWarning(s"${logPrefix}Transformed query:" +
>             s"\nOriginal Plan:\n$plan\nTransformed Plan:\n$newPlan")
>         }
>         newPlan
>       }
>     } else if (conf.isSqlEnabled && conf.isSqlExplainOnlyEnabled) {
>       // this mode logs the explain output and returns the original CPU plan
>       val updatedPlan = updateForAdaptivePlan(plan, conf)
>       GpuOverrides.explainCatalystSQLPlan(updatedPlan, conf)
>       plan
>     } else {
>       plan
>     }
>   }(sparkPlan)
> 
>   private def updateForAdaptivePlan(plan: SparkPlan, conf: RapidsConf): SparkPlan = {
>     if (plan.conf.adaptiveExecutionEnabled) {
>       // AQE can cause Spark to inject undesired CPU shuffles into the plan because GPU and CPU
>       // distribution expressions are not semantically equal.
>       val newPlan = GpuOverrides.removeExtraneousShuffles(plan, conf)
> 
>       // AQE can cause ReusedExchangeExec instance to cache the wrong aggregation buffer type
>       // compared to the desired buffer type from a reused GPU shuffle.
>       GpuOverrides.fixupReusedExchangeExecs(newPlan)
>     } else {
>       plan
>     }
>   }
3152a4372,4466
>   /**
>    *  Determine whether query is running against Delta Lake _delta_log JSON files or
>    *  if Delta is doing stats collection that ends up hardcoding the use of AQE,
>    *  even though the AQE setting is disabled. To protect against the latter, we
>    *  check for a ScalaUDF using a tahoe.Snapshot function and if we ever see
>    *  an AdaptiveSparkPlan on a Spark version we don't expect, fallback to the
>    *  CPU for those plans.
>    *  Note that the Delta Lake delta log checkpoint parquet files are just inefficient
>    *  to have to copy the data to GPU and then back off after it does the scan on
>    *  Delta Table Checkpoint, so have the entire plan fallback to CPU at that point.
>    */
>   def isDeltaLakeMetadataQuery(plan: SparkPlan): Boolean = {
>     val deltaLogScans = PlanUtils.findOperators(plan, {
>       case f: FileSourceScanExec if f.requiredSchema.fields
>          .exists(_.name.startsWith("_databricks_internal")) =>
>         logDebug(s"Fallback for FileSourceScanExec with _databricks_internal: $f")
>         true
>       case f: FileSourceScanExec =>
>         // example filename: "file:/tmp/delta-table/_delta_log/00000000000000000000.json"
>         val found = f.relation.inputFiles.exists { name =>
>           name.contains("/_delta_log/") && name.endsWith(".json")
>         }
>         if (found) {
>           logDebug(s"Fallback for FileSourceScanExec delta log: $f")
>         }
>         found
>       case rdd: RDDScanExec =>
>         // example rdd name: "Delta Table State #1 - file:///tmp/delta-table/_delta_log" or
>         // "Scan ExistingRDD Delta Table Checkpoint with Stats #1 -
>         // file:///tmp/delta-table/_delta_log"
>         val found = rdd.inputRDD != null &&
>           rdd.inputRDD.name != null &&
>           (rdd.inputRDD.name.startsWith("Delta Table State")
>             || rdd.inputRDD.name.startsWith("Delta Table Checkpoint")) &&
>           rdd.inputRDD.name.endsWith("/_delta_log")
>         if (found) {
>           logDebug(s"Fallback for RDDScanExec delta log: $rdd")
>         }
>         found
>       case aqe: AdaptiveSparkPlanExec if 
>         !AQEUtils.isAdaptiveExecutionSupportedInSparkVersion(plan.conf) =>
>         logDebug(s"AdaptiveSparkPlanExec found on unsupported Spark Version: $aqe")
>         true
>       case project: ProjectExec if
>         !AQEUtils.isAdaptiveExecutionSupportedInSparkVersion(plan.conf) =>
>         val foundExprs = project.expressions.flatMap { e =>
>           PlanUtils.findExpressions(e, {
>             case udf: ScalaUDF =>
>               val contains = udf.function.getClass.getCanonicalName.contains("tahoe.Snapshot")
>               if (contains) {
>                 logDebug(s"Found ScalaUDF with tahoe.Snapshot: $udf," +
>                   s" function class name is: ${udf.function.getClass.getCanonicalName}")
>               }
>               contains
>             case _ => false
>           })
>         }
>         if (foundExprs.nonEmpty) {
>           logDebug(s"Project with Snapshot ScalaUDF: $project")
>         }
>         foundExprs.nonEmpty
>       case _ =>
>         false
>     })
>     deltaLogScans.nonEmpty
>   }
> 
>   private def applyOverrides(plan: SparkPlan, conf: RapidsConf): SparkPlan = {
>     val wrap = GpuOverrides.wrapAndTagPlan(plan, conf)
>     if (conf.isDetectDeltaLogQueries && isDeltaLakeMetadataQuery(plan)) {
>       wrap.entirePlanWillNotWork("Delta Lake metadata queries are not efficient on GPU")
>     }
>     val reasonsToNotReplaceEntirePlan = wrap.getReasonsNotToReplaceEntirePlan
>     if (conf.allowDisableEntirePlan && reasonsToNotReplaceEntirePlan.nonEmpty) {
>       if (conf.shouldExplain) {
>         logWarning("Can't replace any part of this plan due to: " +
>             s"${reasonsToNotReplaceEntirePlan.mkString(",")}")
>       }
>       plan
>     } else {
>       val optimizations = GpuOverrides.getOptimizations(wrap, conf)
>       wrap.runAfterTagRules()
>       if (conf.shouldExplain) {
>         wrap.tagForExplain()
>         val explain = wrap.explain(conf.shouldExplainAll)
>         if (explain.nonEmpty) {
>           logWarning(s"\n$explain")
>           if (conf.optimizerShouldExplainAll && optimizations.nonEmpty) {
>             logWarning(s"Cost-based optimizations applied:\n${optimizations.mkString("\n")}")
>           }
>         }
>       }
>       GpuOverrides.doConvertPlan(wrap, conf, optimizations)
>     }
>   }
