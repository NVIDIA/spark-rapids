1c1
< final class CastExprMeta[INPUT <: Cast](
---
> final class CastExprMeta[INPUT <: CastBase](
5c5
<     parent: Option[RapidsMeta[_, _]],
---
>     parent: Option[RapidsMeta[_, _, _]],
21,22c21
<   // 2.x doesn't have the SQLConf.LEGACY_COMPLEX_TYPES_TO_STRING config, so set it to true
<   val legacyCastToString: Boolean = true
---
>   val legacyCastToString: Boolean = SparkShimImpl.getLegacyComplexTypeToString()
46c45
<         if (dt.precision > GpuOverrides.DECIMAL128_MAX_PRECISION) {
---
>         if (dt.precision > DType.DECIMAL128_MAX_PRECISION) {
48c47
<               s"precision > ${GpuOverrides.DECIMAL128_MAX_PRECISION} is not supported yet")
---
>               s"precision > ${DType.DECIMAL128_MAX_PRECISION} is not supported yet")
81a81
>         YearParseUtil.tagParseStringAsDate(conf, this)
83c83
<         // NOOP for anything prior to 3.2.0
---
>         YearParseUtil.tagParseStringAsDate(conf, this)
85,91c85
<         // Spark 2.x: removed check for
<         // !SparkShimImpl.isCastingStringToNegDecimalScaleSupported
<         // this dealt with handling a bug fix that is only in newer versions of Spark
<         // (https://issues.apache.org/jira/browse/SPARK-37451)
<         // Since we don't know what version of Spark 3 they will be using
<         // just always say it won't work and they can hopefully figure it out from warning.
<         if (dt.scale < 0) {
---
>         if (dt.scale < 0 && !SparkShimImpl.isCastingStringToNegDecimalScaleSupported) {
124a119
> 
